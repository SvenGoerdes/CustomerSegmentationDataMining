{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<h3 align=\"center\">Data Mining 2024-25</h3>**\n",
    "## **<h3 align=\"center\">Customer Segmentation - ABCDEats Inc.</h3>**\n",
    "\n",
    "\n",
    "**Group 10 members:**<br>Alexandra Pinto - 20211599@novaims.unl.pt - 20211599<br>\n",
    "Marco Galão  - r20201545@novaims.unl.pt - r20201545<br>\n",
    "Sven Goerdes - 20240503@novaims.unl.pt - 20240503<br>\n",
    "Tim Straub  - 20240505@novaims.unl.pt - 20240505<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"toc\"></a>\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "* [1. Import the Libraries](#import_libraries)\n",
    "* [2. Import the Dataset](#import_dataset)\n",
    "* [3. Description of the Dataset’s Structure](#dataset_structure)\n",
    "* [4. Exploring the Dataset](#exploration)\n",
    "    * [4.1. Constant Features](#constant_features)\n",
    "    * [4.2. Duplicates](#duplicates)\n",
    "    * [4.3. Missing Values](#missing_values)\n",
    "    * [4.4. Data Types](#data_types)\n",
    "    * [4.5. Coherence Checking](#coherence_checking)\n",
    "        * [4.5.1. Minors Customers](#sub_section_4_5_1)\n",
    "        * [4.5.2. Is_chain variable](##sub_section_4_5_2)\n",
    "        * [4.5.3. CUI_Asian vs Japonese, Chinese](#sub_section_4_5_3)\n",
    "        * [4.5.4. Last_order and First_order consistency](#sub_section_4_5_4)\n",
    "        * [4.5.5. Sum DOWs vs sum HRs](##sub_section_4_5_5)\n",
    "        * [4.5.6. Total Orders and Vendor Count consistency ](#sub_section_4_5_6)\n",
    "        * [4.5.7. Total rows with inconsistencies](#sub_section_4_5_7)\n",
    "    * [4.6. Visualizations](#visualizations)\n",
    "    * [4.7. Correlation Matrix](#correlation_matrix)\n",
    "\n",
    "### below just copied from Xana\n",
    "* [4.8. Outliers](#outliers)\n",
    "* [5. Feature Engineering](#feature_engineering)\n",
    "    * [5.1. Total Cuisine Spending](#total_cuisine_spending)\n",
    "    * [5.2. Weekend and Weekday Orders](#weekend_and_weekday_orders)\n",
    "    * [5.3. Generation](#generation)\n",
    "    * [5.4. Hourly Orders Aggregation](#hourly_orders_aggregation)\n",
    "    * [5.5. Healthiness Index](#healthiness_index)\n",
    "    * [5.6. Last Promotion Indicator](#last_promotion_indicator)\n",
    "\n",
    "### below  its features created by Marco\n",
    "\n",
    "* [5. Feature Engineering](#feature_engineering)\n",
    "    * [5.7. Vendor Diversity](#Vendor_Diversity)\n",
    "    * [5.8. Average Daily Orders](#Average_Daily_Orders)\n",
    "    * [5.9. Order Recency](#Order_Recency)\n",
    "    * [5.10. Average Order Value](#Average_Order_Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1. Import the Libraries <a class=\"anchor\" id=\"import_libraries\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "For tasks involving data manipulation, numerical calculations, visualization, and machine learning, imports of libraries like the mentioned in the below cell are crucial. These libraries offer the required functions and tools for preprocessing data, pattern analysis, consumer segmentation, and development of focused marketing campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2. Import the Dataset <a class=\"anchor\" id=\"import_dataset\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "In this section, we import the datasets and set the customer_id as the index column in DM2425_ABCDEats_DATASET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"../Data/DM2425_ABCDEats_DATASET.csv\", index_col=\"customer_id\")\n",
    "df_original.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.tail().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Description of the Dataset’s Structure<a class=\"anchor\" id=\"dataset_structure\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "The <span style=\"color:Blue\"> info()  </span> method prints information about the DataFrame. Using this method we can also see that there are some missing values.\n",
    "*Notes*:\n",
    "- customer_age has some missing values\n",
    "- first_order has some missing values\n",
    "- HR_0 has some missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- customer age should be a integer\n",
    "- first order is number of days so should be a integer\n",
    "- HR_0: convert to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "The <span style=\"color:Blue\"> describe()  </span> method  is used for calculating some statistical data. The features: *customer_name*, *customer_gender*, and *customer_birthdate* don\"t have any statistical data since they\"re categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.describe().round(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the HR_0 is always equal to zero, so we can delete this features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original[\"customer_region\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original[\"last_promo\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets replace the - for None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original[\"payment_method\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"exploration\"></a>\n",
    "\n",
    "# 4. Exploring the Dataset <a class=\"anchor\" id=\"a\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "In this section we perform several checks and treatments to ensure data quality. <br>\n",
    "This entails locating and managing duplicates to prevent redundant data, removing constant characteristics that don\"t offer insightful information, and dealing with missing values using methods like imputation or exclusion. These procedures are essential for preserving data integrity and enhancing the accuracy of later analysis and modeling efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.1. Constant features <a class=\"anchor\" id=\"constant_features\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>\n",
    "\n",
    "The <span style='color:Blue'> var() </span> method allows us to check if there are any of the numerical variables are univariate (variance is equal to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = df_original.var(numeric_only=True)\n",
    "variances[variances == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_hr0 = df_original.drop(\"HR_0\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.2. Duplicates <a class=\"anchor\" id=\"duplicates\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>\n",
    "\n",
    "With the <span style=\"color:Blue\"> drop_duplicates() </span> method the duplicates rows will be dropped. We can conclude that that this dataset had 60 duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicated observations\n",
    "df_no_hr0.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure data integrity by maintaining the integrity of the original data, reproducibility by enabling iterative analysis and validation of the transformations without changing the original data, it is necessary to make a copy of the original dataset before applying transformations, for that we use the <span style=\"color:Blue\"> copy()  </span> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates = df_no_hr0.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"HR_0\" in df_no_hr0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicated indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.3. Missing values <a class=\"anchor\" id=\"missing_values\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we conclude there are some features that have some missing values. In this subsection we will check for each feature and try to understand what would be the best approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing = df_no_duplicates.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage of missing values in each column\n",
    "perc_missing_values = (df_treat_missing.isna().sum() / len(df_treat_missing) * 100).round(2)\n",
    "perc_missing_values[perc_missing_values > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the percentage of disguised missing values \"-\" in each column\n",
    "perc_dash_values = (df_treat_missing.apply(lambda x: (x == '-').sum()) / len(df_treat_missing) * 100).round(2)\n",
    "perc_dash_values[perc_dash_values > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many observations have missing values in more than 1 variable (there are 7 customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_missing_values = [\"customer_age\", \"customer_region\", \"first_order\"]\n",
    "df_treat_missing[columns_missing_values].isna().sum(axis=1).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**last_promo**\n",
    "\n",
    "Change '-' of last_promo to 'NoPromo' as it is not a missing value but rather indicates that the customer never used a promo before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing[\"last_promo\"].replace(\"-\", 'NOPROMO', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing[\"last_promo\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**customer_region**\n",
    "\n",
    "\n",
    "\n",
    "Notes:\n",
    "- Values could represent postal codes\n",
    "- So customer region values with the same prefix indicate geographical proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing[\"customer_region\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify the cuisine-related columns (assuming they are named like 'CUI_American', 'CUI_Asian', etc.)\n",
    "cuisine_columns = [col for col in df_treat_missing.columns if col.startswith('CUI_')]\n",
    "\n",
    "# Step 2: Define a consistent color palette for the cuisines\n",
    "colors = plt.cm.get_cmap('tab20', len(cuisine_columns)).colors\n",
    "cuisine_color_map = {cuisine: colors[i] for i, cuisine in enumerate(cuisine_columns)}\n",
    "cuisine_color_map['Others'] = 'gray'  # Color for \"Others\"\n",
    "\n",
    "# Step 3: Group by 'customer_region' and sum the spending across cuisines\n",
    "region_cuisine_spending = df_treat_missing.groupby('customer_region')[cuisine_columns].sum()\n",
    "\n",
    "# Step 4: Setup the 3x3 grid for pie charts\n",
    "regions = region_cuisine_spending.index\n",
    "num_cols = 3\n",
    "num_rows = 3  \n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "\n",
    "# Step 5: Plot a pie chart for each region, only showing top 5 cuisines and aggregating the rest\n",
    "for i, region in enumerate(regions):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Sort the cuisines by spending and get the top 5\n",
    "    region_data = region_cuisine_spending.loc[region].sort_values(ascending=False)\n",
    "    top_5_cuisines = region_data.head(5)\n",
    "    other_cuisines = region_data[5:].sum()\n",
    "\n",
    "    # Combine top 5 with \"Others\" if necessary\n",
    "    if other_cuisines > 0:\n",
    "        top_5_cuisines['Others'] = other_cuisines\n",
    "\n",
    "    # Prepare labels with percentages outside of the pie chart\n",
    "    labels = [f'{cuisine} ({value:.1f}%)' for cuisine, value in zip(top_5_cuisines.index, (top_5_cuisines / top_5_cuisines.sum()) * 100)]\n",
    "    \n",
    "    # Pie chart for the current region with consistent colors\n",
    "    ax.pie(top_5_cuisines, labels=labels, startangle=90, colors=[cuisine_color_map[cuisine] for cuisine in top_5_cuisines.index], wedgeprops=dict(width=0.3))\n",
    "    ax.set_title(f'Region {region}', fontsize=14, fontweight='bold')\n",
    "\n",
    "# If there are empty subplots, hide them\n",
    "for i in range(9, num_rows * num_cols):\n",
    "    fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "# Step 6: Create a legend with cuisine color mapping\n",
    "fig.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=cuisine_color_map[cuisine], markersize=10) for cuisine in cuisine_color_map],\n",
    "           labels=list(cuisine_color_map.keys()), loc='upper center', title='Cuisines', ncol=5, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "# Step 7: Customize and display the plot\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Top 5 Cuisine Spending Distribution by Region', fontsize=16)\n",
    "plt.subplots_adjust(top=0.85)  # Adjust title positioning and layout for the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strong correlation here between the emtpy region \"-\" (top left) and the region 8670 (bottom right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the cuisine-related columns\n",
    "cuisine_columns = [col for col in df_treat_missing.columns if col.startswith('CUI_')]\n",
    "\n",
    "# Group by 'customer_region' and sum the spending across all cuisines\n",
    "total_spending_by_region = df_treat_missing.groupby('customer_region')[cuisine_columns].sum().sum(axis=1)\n",
    "\n",
    "#now divide the spending by the number of customers in each region\n",
    "total_customers_by_region = df_treat_missing['customer_region'].value_counts()\n",
    "spending_per_customer = total_spending_by_region / total_customers_by_region\n",
    "\n",
    "#plot the results\n",
    "spending_per_customer.plot(kind='bar', style='o-', color='skyblue', figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also for average spending per customer the two null region is the most similar to 8670"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the two regions directly with each other (including another possible region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get spending data for the regions\n",
    "region_dash_spending = region_cuisine_spending.loc['-']\n",
    "region_8370_spending = region_cuisine_spending.loc['8370']\n",
    "region_8670_spending = region_cuisine_spending.loc['8670']\n",
    "\n",
    "# Step 2: Sort the new region and get the top 5 cuisines\n",
    "top_5_cuisines_dash = region_dash_spending.sort_values(ascending=False).head(5)\n",
    "top_5_cuisines_8370 = region_8370_spending.sort_values(ascending=False).head(5)\n",
    "top_5_cuisines_8670 = region_8670_spending.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Step 3: Calculate the percentages of spending for region \"8370\"\n",
    "top_5_cuisines_dash_percentage = (top_5_cuisines_dash / top_5_cuisines_dash.sum()) * 100\n",
    "top_5_cuisines_8370_percentage = (top_5_cuisines_8370 / top_5_cuisines_8370.sum()) * 100\n",
    "top_5_cuisines_8670_percentage = (top_5_cuisines_8670 / top_5_cuisines_8670.sum()) * 100\n",
    "\n",
    "# Step 4: Combine all the top cuisines to find the common ones\n",
    "all_cuisines = set(top_5_cuisines_dash_percentage.index) | set(top_5_cuisines_8670_percentage.index) | set(top_5_cuisines_8370_percentage.index)\n",
    "\n",
    "# Step 5: Recalculate percentages including any missing cuisines (fill missing with 0)\n",
    "top_5_cuisines_dash_percentage = top_5_cuisines_dash_percentage.reindex(all_cuisines, fill_value=0)\n",
    "top_5_cuisines_8670_percentage = top_5_cuisines_8670_percentage.reindex(all_cuisines, fill_value=0)\n",
    "top_5_cuisines_8370_percentage = top_5_cuisines_8370_percentage.reindex(all_cuisines, fill_value=0)\n",
    "\n",
    "# Step 6: Create a DataFrame to compare all three regions\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Region \"8670\" Percentage': top_5_cuisines_8670_percentage,\n",
    "    'Region \"-\" Percentage': top_5_cuisines_dash_percentage,\n",
    "    'Region \"8370\" Percentage': top_5_cuisines_8370_percentage\n",
    "})\n",
    "\n",
    "# Step 7: Plot the comparison for visualization\n",
    "comparison_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Comparison of Top 5 Cuisines: Region \"-\" vs Region \"8670\" and Region \"8370\"')\n",
    "plt.ylabel('Percentage of Total Spending')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Calculate the percentage differences between Region \"-\" and the other two regions\n",
    "percentage_difference_8670 = (top_5_cuisines_dash_percentage - top_5_cuisines_8670_percentage).abs()\n",
    "percentage_difference_8370 = (top_5_cuisines_dash_percentage - top_5_cuisines_8370_percentage).abs()\n",
    "\n",
    "# Step 9: Create a DataFrame to show the differences\n",
    "difference_df = pd.DataFrame({\n",
    "    'Cuisine': percentage_difference_8670.index,\n",
    "    'Difference with Region \"8670\" (%)': percentage_difference_8670.values,\n",
    "    'Difference with Region \"8370\" (%)': percentage_difference_8370.values\n",
    "})\n",
    "\n",
    "difference_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers indicate the customers of the empty region are in close geographical proximity to 8670. Therefore we will change the region for all customers of \"-\" to 8670."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing['customer_region'].replace('-', '8670', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**customer_age**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for similarities between NaN value users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to include only rows where customer_age is NaN\n",
    "nan_age_df = df_treat_missing[df_treat_missing['customer_age'].isna()]\n",
    "\n",
    "# Create a dictionary to store the most common value and its percentage for each feature\n",
    "common_values = {}\n",
    "\n",
    "# For each column in the filtered DataFrame, calculate the most common value and its percentage\n",
    "for column in nan_age_df.columns:\n",
    "    if nan_age_df[column].notna().sum() > 0:  # Check if there are non-NaN values in the column\n",
    "        most_common_value = nan_age_df[column].mode()[0]  # Get the most frequent value\n",
    "        percentage = (nan_age_df[column].value_counts(normalize=True).iloc[0]) * 100  # Calculate its percentage\n",
    "        common_values[column] = [most_common_value, percentage]\n",
    "    else:\n",
    "        # If the column only contains NaN values, mark it as such\n",
    "        common_values[column] = ['NaN only', 0]\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easy visualization\n",
    "common_values_df = pd.DataFrame(common_values, index=['Most Common Value', 'Percentage (%)']).T\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(common_values_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No feature is really outstanding here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the generation feature (see Feature Engineering) for ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels for the generation categories\n",
    "bins = [14, 25, 41, 57, 76, 81]  # Age ranges\n",
    "labels = [\"Gen_Z\", \"Millennials\", \"Gen_X\", \"Baby_Boomers\", \"Silent_Generation\"]\n",
    "\n",
    "# Step 1: Create a new column for generation based on customer_age, ignoring NaNs for now\n",
    "df_treat_missing[\"generation\"] = pd.cut(df_treat_missing[\"customer_age\"], bins=bins, labels=labels)\n",
    "\n",
    "# Step 2: Convert the generation column to a categorical type and add 'Unknown' as a category\n",
    "df_treat_missing[\"generation\"] = df_treat_missing[\"generation\"].astype('category')\n",
    "df_treat_missing[\"generation\"] = df_treat_missing[\"generation\"].cat.add_categories('Unknown')\n",
    "\n",
    "# Step 3: Fill NaN values in the 'generation' column with 'Unknown'\n",
    "df_treat_missing[\"generation\"].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Step 4: Check the value counts for the generation column (including 'Unknown')\n",
    "df_treat_missing[\"generation\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Step 1: Add a category for NaN values in the 'generation' column\n",
    "# df_treat_missing['generation'] = pd.cut(df_treat_missing['customer_age'], bins=bins, labels=labels)\n",
    "# df_treat_missing['generation'] = df_treat_missing['generation'].cat.add_categories('Unknown')\n",
    "# df_treat_missing['generation'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# # Step 2: Identify the cuisine-related columns (assuming they are named like 'CUI_American', 'CUI_Asian', etc.)\n",
    "# cuisine_columns = [col for col in df_treat_missing.columns if col.startswith('CUI_')]\n",
    "\n",
    "# # Step 3: Group by 'generation' and sum the spending across cuisines\n",
    "# generation_cuisine_spending = df_treat_missing.groupby('generation')[cuisine_columns].sum()\n",
    "\n",
    "# # Step 4: Plot top 5 spending cuisines for each generation\n",
    "# generations = generation_cuisine_spending.index\n",
    "# num_generations = len(generations)\n",
    "\n",
    "# # Create a 2x3 grid for the pie charts (to accommodate the different generations)\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# for i, generation in enumerate(generations):\n",
    "#     row = i // 3\n",
    "#     col = i % 3\n",
    "#     ax = axes[row, col]\n",
    "\n",
    "#     # Sort the cuisines by spending and get the top 5\n",
    "#     generation_data = generation_cuisine_spending.loc[generation].sort_values(ascending=False)\n",
    "#     top_5_cuisines = generation_data.head(5)\n",
    "#     other_cuisines = generation_data[5:].sum()\n",
    "\n",
    "#     # Combine top 5 with \"Others\" if necessary\n",
    "#     if other_cuisines > 0:\n",
    "#         top_5_cuisines['Others'] = other_cuisines\n",
    "\n",
    "#     # Prepare labels with percentages for the top 5 cuisines\n",
    "#     labels = [f'{cuisine} ({value:.1f}%)' for cuisine, value in zip(top_5_cuisines.index, (top_5_cuisines / top_5_cuisines.sum()) * 100)]\n",
    "\n",
    "#     # Pie chart for the current generation\n",
    "#     ax.pie(top_5_cuisines, labels=labels, startangle=90, autopct='%1.1f%%')\n",
    "#     ax.set_title(f'{generation} Spending', fontsize=14, fontweight='bold')\n",
    "\n",
    "# # Remove empty subplots (if fewer than 6 generations)\n",
    "# for i in range(num_generations, 6):\n",
    "#     fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "# # Step 5: Customize and display the plot\n",
    "# plt.tight_layout()\n",
    "# plt.suptitle('Top 5 Spending Cuisines by Generation', fontsize=16)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the unique generations including 'Unknown' for NaN values, excluding 'Baby_Boomers' and 'Silent_Generation'\n",
    "# generations = df_treat_missing['generation'].unique()\n",
    "# generations = [gen for gen in generations if gen not in ['Baby_Boomers', 'Silent_Generation']]  # Exclude unwanted generations\n",
    "\n",
    "# # Create a bar chart for each generation separately, including the 'Unknown' group\n",
    "# for generation in generations:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "\n",
    "#     # Filter data for the current generation, including 'Unknown'\n",
    "#     generation_data = df_treat_missing[df_treat_missing['generation'] == generation]\n",
    "\n",
    "#     # Check if there is data to plot\n",
    "#     if not generation_data['first_order'].isna().all():  # Ensure there is data\n",
    "#         # Get the value counts of 'first_order' and normalize them by dividing by the total count for that generation\n",
    "#         normalized_counts = generation_data['first_order'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "#         # Plot the normalized frequency distribution of 'first_order' for the current generation\n",
    "#         normalized_counts.plot(kind='bar', color='skyblue')\n",
    "\n",
    "#         # Add titles and labels\n",
    "#         title = f'First Order Distribution (Normalized) for Generation {generation}' if pd.notna(generation) else 'First Order Distribution (Normalized) for Unknown'\n",
    "#         plt.title(title, fontsize=16)\n",
    "#         plt.xlabel('First Order Entry')\n",
    "#         plt.ylabel('Proportion')\n",
    "\n",
    "#         # Show the bar chart for each generation\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print(f\"No data to plot for Generation {generation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No imputations can be created out of the data exploration. As the null values are only 2.28 I recommend dropping the customers with null values for the age. Nonetheless, will I impute the missing values for the data visualization using the median age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_age = df_treat_missing['customer_age'].median()\n",
    "df_treat_missing['customer_age'].fillna(median_age, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# import pandas as pd\n",
    "\n",
    "# target_column = df_treat_missing['customer_age'].drop()\n",
    "\n",
    "# # Step 2: Identify numerical and categorical columns in other features\n",
    "# numerical_cols = df_treat_missing.select_dtypes(include=['number']).columns\n",
    "# categorical_cols = df_treat_missing.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# # Step 3: Apply ordinal encoding to the categorical columns without changing their names\n",
    "# encoder = OrdinalEncoder()\n",
    "# encoded_categorical = pd.DataFrame(encoder.fit_transform(df_treat_missing[categorical_cols]), \n",
    "#                                    index=df_treat_missing.index, \n",
    "#                                    columns=categorical_cols)  # Keep the original column names\n",
    "\n",
    "# # Step 4: Combine numerical and encoded categorical data (excluding customer_age)\n",
    "# numerical_data = df_treat_missing[numerical_cols]\n",
    "# combined_data = pd.concat([numerical_data, encoded_categorical], axis=1)\n",
    "\n",
    "# combined_data.head()\n",
    "\n",
    "# # Step 5: Add the customer_age column back (with missing values)\n",
    "# combined_data['customer_age'] = df_treat_missing['customer_age']\n",
    "\n",
    "# # Ensure all column names are strings (necessary for sklearn imputation)\n",
    "# combined_data.columns = combined_data.columns.astype(str)\n",
    "\n",
    "# # Step 6: Impute missing values in customer_age using kNN\n",
    "# imputer = KNNImputer(n_neighbors=5)\n",
    "# imputed_data = imputer.fit_transform(combined_data)\n",
    "\n",
    "# # Step 7: Convert imputed data back into a DataFrame and update customer_age\n",
    "# imputed_df = pd.DataFrame(imputed_data, columns=combined_data.columns)\n",
    "# imputed_df.head()\n",
    "\n",
    "# # Step 8: Check if there are any remaining missing values in customer_age\n",
    "# print(imputed_df['customer_age'].isna().sum())  # Should print 0 if all missing values are filled\n",
    "\n",
    "# # Optional: Update the original dataframe with imputed values\n",
    "# df_treat_missing['customer_age'] = imputed_df['customer_age']\n",
    "\n",
    "# # Display first few rows of the updated dataframe\n",
    "# df_treat_missing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**first_order**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring correalation between NaN values and order amounts and last_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_first_order = df_treat_missing[df_treat_missing['first_order'].isnull()]\n",
    "print(f'Amount of null values for first_order: {df_null_first_order.shape[0]}\\nCustomers who only ordered from 1 vendor: {df_null_first_order[(df_null_first_order['vendor_count']  ==1)].shape[0]}\\nCustomers that have a last_order value of 0: {df_null_first_order[(df_null_first_order['last_order']  == 0)].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_first_order[df_null_first_order['vendor_count'] > 1].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "104 out of 106 NaN values only ordered from one vendor. The two that ordered from 2 vendors ordered it on the same DOW.\n",
    "This is a strong indication that the missing values are occuring when the first and last order are the same and at the point when the dataset was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing[(df_treat_missing['first_order'] == 0) & (df_treat_missing['last_order'] == 0)].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No occurence of a customer with first and last_order = 0 underlines the previous indications\n",
    "\n",
    "Imputation: Fill the NaN values in first_order with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing['first_order'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After imputing the missing values create the df for further use in the following tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates = df_treat_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.4. Data Types <a class=\"anchor\" id=\"data_types\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_dtypes = df_treat_missing.copy()\n",
    "df_adjust_dtypes.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- age: should be a integer\n",
    "- first order: is number of days so should be a integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using \"Int64\" allows for the representation of NaN values alongside integers\n",
    "# df_adjust_dtypes[\"customer_age\"] = df_adjust_dtypes[\"customer_age\"].astype(\"Int64\")\n",
    "# df_adjust_dtypes[\"first_order\"] = df_adjust_dtypes[\"first_order\"].astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_dtypes.dtypes[[\"customer_age\", \"first_order\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.5. Coherence Checking <a class=\"anchor\" id=\"coherence_checking\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>\n",
    "\n",
    "> ### 4.5.1. Minors Customers <a class=\"anchor\" id=\"sub_section_4_5_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inconsistencies = df_adjust_dtypes.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if there are minor customers (should not exist, because they are not allowed to placed orders online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the age distribution\n",
    "df_inconsistencies[\"customer_age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count customers under 18\n",
    "minor_customers = df_inconsistencies[df_inconsistencies[\"customer_age\"] < 18]\n",
    "perc_minor_customers = len(minor_customers) / len(df_inconsistencies) * 100\n",
    "\n",
    "print(f\"Number of customers under 18 years old: {len(minor_customers)} ({round(perc_minor_customers, 2)}%)\")\n",
    "\n",
    "minor_customers.describe().round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_customers.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that only 1.14% of your customers are minors we can decide to remove these customers. Removing these customers can lead to more accurate analysis and insights relevant to the target demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove customers under 18 from the dataset\n",
    "df_inconsistencies = df_inconsistencies[df_inconsistencies[\"customer_age\"] >= 18]\n",
    "\n",
    "# Verify the removal\n",
    "remaining_minor_customers = df_inconsistencies[df_inconsistencies[\"customer_age\"] < 18]\n",
    "print(f\"Remaining customers under 18 years old: {len(remaining_minor_customers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.2. Is_chain variable <a class=\"anchor\" id=\"sub_section_4_5_2\"></a>\n",
    "Explore \"is_chain\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inconsistencies[\"is_chain\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the metadata, \"is_chain\" - \"Indicates whether the customer’s order was from a chain restaurant.\". From this statistics, sure the metadata is wrong. However, as we cannot guess any possible meaning for this variable, we decide to delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inconsistencies.drop(\"is_chain\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.3. CUI_Asian vs Japonese, Chinese <a class=\"anchor\" id=\"sub_section_4_5_3\"></a>\n",
    "\n",
    "The classification of restaurants into broad and specific cuisine categories (such as CUI_Asian alongside CUI_Chinese, CUI_Indian, CUI_Japanese, etc.) could lead to inconsistencies and overlaps in data. Some restaurants may be categorized under both a broad category (CUI_Asian) and a specific one (e.g., CUI_Chinese), while others might only fall under the broader classification. This double classification could result in redundancy, making it challenging to analyze customer preferences and spending habits accurately.\n",
    "\n",
    "To explore this issue, we can analyze the following:\n",
    "\n",
    "- Frequency of Overlap: How often do entries fall under both CUI_Asian and specific categories?\n",
    "- Correlation Analysis: How strongly correlated are the specific categories with the broader CUI_Asian category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temporary feature for the sum of specific Asian cuisines\n",
    "\n",
    "df_temp= df_inconsistencies.copy()\n",
    "\n",
    "# Update the cuisine_categories list to include the new temporary feature\n",
    "asian_cui_categories = ['CUI_Asian', 'CUI_Chinese', 'CUI_Indian', 'CUI_Japanese', 'CUI_Noodle Dishes', 'CUI_Thai']\n",
    "\n",
    "# Create a binary matrix of cuisine presence (1 if there's spending, 0 otherwise)\n",
    "df_cuisines = df_temp[asian_cui_categories] > 0\n",
    "\n",
    "# Calculate overlap with CUI_Asian\n",
    "overlap_counts = {}\n",
    "for category in asian_cui_categories[1:]:  # Skip CUI_Asian\n",
    "    overlap_counts[category] = df_cuisines[(df_cuisines['CUI_Asian']) & (df_cuisines[category])].shape[0]\n",
    "\n",
    "# Convert both overlap counts and non-zero counts to a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Cuisine': overlap_counts.keys(),\n",
    "    'Asian_Overlap_Count': overlap_counts.values(),\n",
    "    'Total_Non_Zero_Count': [non_zero_counts[cat] for cat in overlap_counts.keys()]\n",
    "})\n",
    "\n",
    "# Calculate the proportion of overlap compared to total non-zero counts\n",
    "comparison_df['Overlap_Proportion'] = comparison_df['Asian_Overlap_Count'] / comparison_df['Total_Non_Zero_Count']\n",
    "\n",
    "# Display the DataFrame\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the indices of exact matches with CUI_Asian across rows (excluding zero matches)\n",
    "all_matches_indices = {}\n",
    "\n",
    "# Loop through each specific cuisine category (excluding CUI_Asian itself)\n",
    "for category in asian_cui_categories[1:]:  # Starting from the specific cuisines\n",
    "    # Find rows where the values are exactly equal and both are non-zero\n",
    "    matches = (df_temp[category] == df_temp['CUI_Asian']) & (df_temp[category] != 0)\n",
    "    # Store the indices where this condition is True\n",
    "    all_matches_indices[category] = df_temp[matches].index.tolist()\n",
    "\n",
    "# Print the indices for each cuisine\n",
    "for cuisine, indices in all_matches_indices.items():\n",
    "    print(f\"Indices where {cuisine} value is equal to CUI_Asian (excluding zeros): {indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.loc['3d98e75f8a'][asian_cui_categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.loc['c97280b5af'][asian_cui_categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.loc['85e24db1d8'][asian_cui_categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis between CUI_Asian and specific categories\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = df_inconsistencies[asian_cui_categories].corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix,mask=mask, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix: CUI_Asian and Specific Cuisines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.4. Last_order and First_order consistency <a class=\"anchor\" id=\"sub_section_4_5_4\"></a>\n",
    "\n",
    "Verify that last_order is lower than or equal to first_order for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in \"first_order\" (\"last_order\" has no missing values)\n",
    "data_cleaned = df_inconsistencies.dropna(subset=\"first_order\")\n",
    "\n",
    "# Check when \"last_order\" is greater than \"first_order\"\n",
    "is_last_order_greater = data_cleaned[\"last_order\"] > data_cleaned[\"first_order\"]\n",
    "\n",
    "# Calculate the percentage of inconsistent rows based on the original dataframe\n",
    "perc_inconsistencies = is_last_order_greater.sum() / len(df_inconsistencies) * 100\n",
    "\n",
    "# Output the percentage of rows where \"last_order\" is greater than \"first_order\"\n",
    "print(f\"Inconsistent rows: {perc_inconsistencies.round(2)}%\")\n",
    "\n",
    "# Calculate the differences between \"last_order\" and \"first_order\"\n",
    "diffs = data_cleaned[\"last_order\"] - data_cleaned[\"first_order\"]\n",
    "\n",
    "# Display how many rows have certain differences\n",
    "print(\"\\nCounts of differences between last_order and first_order:\")\n",
    "diffs.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible approach: Since 77.11% of the entries have last_order greater than first_order, this suggests a serious issue with these columns. If the inconsistencies cannot be resolved, it may be better to remove them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows where the sums are inconsistent\n",
    "inconsistencies = df_inconsistencies[\"last_order\"] > df_inconsistencies[\"first_order\"]\n",
    "\n",
    "# Get summary statistics of the inconsistent rows\n",
    "df_inconsistencies[inconsistencies].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inconsistencies[inconsistencies].describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.5. Sum DOWs vs sum HRs <a class=\"anchor\" id=\"sub_section_4_5_5\"></a>\n",
    "\n",
    "Check that the sum of orders by day (DOW_0 to DOW_6) matches the total number of orders from hourly data (HR_1 to HR_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of DOW columns (DOW_0 to DOW_6)\n",
    "dow_columns = [f\"DOW_{i}\" for i in range(7)]\n",
    "dow_sum = df_inconsistencies[dow_columns].sum(axis=1)\n",
    "\n",
    "# Sum of HR columns (HR_0 to HR_23, including HR_0)\n",
    "hr_columns = [f\"HR_{i}\" for i in range(1, 24)]\n",
    "hr_sum = df_inconsistencies[hr_columns].sum(axis=1)\n",
    "\n",
    "# Check if the sums are equal\n",
    "sums_equal = dow_sum != hr_sum\n",
    "\n",
    "# Output the percentage of rows where DOW sum and HR sum are different\n",
    "perc_inconsistent = sums_equal.sum() / len(df_inconsistencies) * 100\n",
    "print(f\"Inconsistent rows: {perc_inconsistent.round(2)}%\")\n",
    "\n",
    "# Calculate and display the absolute differences\n",
    "sums_diff = (dow_sum - hr_sum).abs()\n",
    "\n",
    "# Display how many rows have certain differences\n",
    "print(\"\\nCounts of differences between dow_sum and hr_sum:\")\n",
    "sums_diff.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible approach:\n",
    "- small inconsistencies (e.g., 1-2 orders): Adjust values (also because they represent the highest amount of inconsistencies).\n",
    "- large inconsistencies: Investigate patterns, and remove if they appear random (also because they represent a very small part of the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\"s analyze the small inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows where the sums are inconsistent\n",
    "df_small_inconsistencies = df_inconsistencies[sums_diff <= 2]\n",
    "\n",
    "# Get summary statistics of the inconsistent rows\n",
    "df_small_inconsistencies.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics of the small inconsistent rows (for categorical variables)\n",
    "df_small_inconsistencies.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's treat the inconsistencies in DOW and HR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small inconsistencies (the \"problem\" is that we'll be converting DOW columns to floating numbers, due to the adjustment factor). Therefore, for now let's just remove these inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify small inconsistencies (1 or 2 orders)\n",
    "# small_inconsistencies_mask = (sums_diff == 1) | (sums_diff == 2)\n",
    "\n",
    "# Identify large inconsistencies (> 2 orders)\n",
    "# large_inconsistencies_mask = sums_diff > 2\n",
    "\n",
    "# Adjust the smaller sum (either DOW or HR) to match the larger one for small inconsistencies\n",
    "\n",
    "# Create a mask for rows with small inconsistencies\n",
    "# small_dow_sum = dow_sum[small_inconsistencies_mask]\n",
    "# small_hr_sum = hr_sum[small_inconsistencies_mask]\n",
    "\n",
    "# Calculate the adjustment factor for small inconsistencies\n",
    "# adjustment_factors = small_hr_sum / small_dow_sum\n",
    "\n",
    "# We broadcast the adjustment factors to the corresponding rows in dow_columns\n",
    "# df_inconsistencies.loc[small_inconsistencies_mask, dow_columns] *= adjustment_factors.values[:, np.newaxis]\n",
    "\n",
    "# Recalculate sums after adjustments to verify the changes\n",
    "# adjusted_dow_sum = df_inconsistencies[dow_columns].sum(axis=1)\n",
    "# adjusted_hr_sum = df_inconsistencies[hr_columns].sum(axis=1)\n",
    "\n",
    "# Update the difference calculation after adjustments\n",
    "# adjusted_sums_diff = (adjusted_dow_sum - adjusted_hr_sum).abs()\n",
    "\n",
    "# Check if small inconsistencies have been successfully treated\n",
    "# small_inconsistencies_treated_count = ((adjusted_sums_diff == 1) | (adjusted_sums_diff == 2)).sum()\n",
    "\n",
    "# Print the count of small inconsistencies after treatment\n",
    "# print(f\"Count of small inconsistencies after treatment: {small_inconsistencies_treated_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the rows with inconsistencies\n",
    "inconsistencies_mask = sums_diff > 0\n",
    "\n",
    "# Remove inconsistent rows from the DataFrame\n",
    "df_inconsistencies = df_inconsistencies[~inconsistencies_mask].copy()\n",
    "\n",
    "# Recalculate the sums for DOW and HR columns after removal\n",
    "cleaned_dow_sum = df_inconsistencies[dow_columns].sum(axis=1)\n",
    "cleaned_hr_sum_cleaned = df_inconsistencies[hr_columns].sum(axis=1)\n",
    "\n",
    "# Calculate the differences again\n",
    "sums_diff_cleaned = (cleaned_dow_sum - cleaned_hr_sum_cleaned).abs()\n",
    "\n",
    "# Check if there are any remaining inconsistencies\n",
    "remaining_inconsistencies_count = (sums_diff_cleaned > 2).sum()\n",
    "\n",
    "# Print results to verify\n",
    "print(f\"Number of remaining inconsistencies after cleaning: {remaining_inconsistencies_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.6. Total Orders and Vendor Count consistency <a class=\"anchor\" id=\"sub_section_4_5_6\"></a>\n",
    "\n",
    "Total Orders Consistency: The total number of orders placed (the sum of the DOW/HR columns) should not exceed the total number of products ordered (product_count).<br>\n",
    "\n",
    "Vendor Count Consistency: The vendor_count should not exceed the number of unique products ordered, since each product should come from a vendor. This ensures that the recorded count of unique vendors matches the reality of product purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total orders based on DOW columns\n",
    "total_orders = df_inconsistencies[dow_columns].sum(axis=1)\n",
    "\n",
    "# Check if total orders are less than product count\n",
    "inconsistent_orders = total_orders > df_inconsistencies['product_count']\n",
    "\n",
    "# Check if vendor count exceeds product count\n",
    "inconsistent_vendors = df_inconsistencies['vendor_count'] > df_inconsistencies['product_count']\n",
    "\n",
    "# Combine both conditions to find all inconsistencies\n",
    "inconsistencies_mask = inconsistent_orders | inconsistent_vendors\n",
    "\n",
    "# Count the number of inconsistent rows\n",
    "num_inconsistent_rows = inconsistencies_mask.sum()\n",
    "\n",
    "# Calculate the percentage of inconsistent rows\n",
    "perc_inconsistent_rows = num_inconsistent_rows / len(df_inconsistencies) * 100\n",
    "\n",
    "# Display results\n",
    "print(f\"Number of inconsistent rows: {num_inconsistent_rows} ({perc_inconsistent_rows.round(2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the rows with inconsistencies in orders and vendors are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the indices of both inconsistency sets are the same\n",
    "inconsistent_rows_same = inconsistent_orders.index.equals(inconsistent_vendors.index)\n",
    "\n",
    "# Step 2: Output the result\n",
    "if inconsistent_rows_same:\n",
    "    print(\"The rows with inconsistencies in orders and vendors are the same.\")\n",
    "else:\n",
    "    print(\"There are differences in the rows with inconsistencies between orders and vendors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inconsistencies[inconsistencies_mask].head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with inconsistencies in total orders or vendor count\n",
    "df_inconsistencies = df_inconsistencies[~inconsistencies_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.7. Total rows with inconsistencies <a class=\"anchor\" id=\"sub_section_4_5_7\"></a>\n",
    "Final Step: Calculate the Percentage of Rows Removed Due to Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows removed\n",
    "rows_removed = len(df_adjust_dtypes) - len(df_inconsistencies)\n",
    "\n",
    "# Calculate the percentage of rows removed\n",
    "percentage_removed = rows_removed / len(df_adjust_dtypes) * 100\n",
    "\n",
    "# Output the result\n",
    "print(f\"Rows removed due to inconsistencies: {rows_removed} ({round(percentage_removed, 2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.6. Visualizations <a class=\"anchor\" id=\"visualizations\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib tutorials: https://matplotlib.org/stable/tutorials/index.html\n",
    "\n",
    "Matplotlib gallery: https://matplotlib.org/stable/gallery/index.html\n",
    "\n",
    "Seaborn tutorials: https://seaborn.pydata.org/tutorial.html\n",
    "\n",
    "Seaborn gallery: https://seaborn.pydata.org/examples/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualizations = df_inconsistencies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualizations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric and non-metric features\n",
    "metric_features = df_visualizations.select_dtypes(include=['number']).columns.tolist()\n",
    "non_metric_features = df_visualizations.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "# Exclude DOW and HR columns from metric features\n",
    "metric_features_excluding_dow = [feat for feat in metric_features if feat not in dow_columns]\n",
    "metric_features_excluding_hr = [feat for feat in metric_features if feat not in hr_columns]\n",
    "\n",
    "# Exclude both DOW and HR columns from metric features\n",
    "metric_features_excluding_dow_and_hr = [feat for feat in metric_features if feat not in dow_columns + hr_columns]\n",
    "\n",
    "# Output the results\n",
    "print(f\"Non-metric features: {non_metric_features}\")\n",
    "print(f\"Metric features (excluding DOW): {metric_features_excluding_dow}\")\n",
    "print(f\"Metric features (excluding HR): {metric_features_excluding_hr}\")\n",
    "print(f\"Metric features (excluding DOW and HR): {metric_features_excluding_dow_and_hr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ALL Numeric Variables' Histograms in one figure\n",
    "\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "# Calculate the number of rows and columns for a square matrix layout\n",
    "total_features = len(metric_features)\n",
    "sp_cols = math.ceil(math.sqrt(total_features))  # Columns based on the square root of total features\n",
    "sp_rows = math.ceil(total_features / sp_cols)   # Rows to accommodate all features\n",
    "\n",
    "# Prepare figure. Create individual axes where each histogram will be placed\n",
    "fig, axes = plt.subplots(sp_rows, \n",
    "                         sp_cols, \n",
    "                         figsize=(20, 11),\n",
    "                         tight_layout=True\n",
    "                        )\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each histogram (hint: use the ax.hist() instead of plt.hist()):\n",
    "for ax, feat in zip(axes.flatten(), metric_features): # Notice the zip() function and flatten() method\n",
    "    ax.hist(df_visualizations[feat])\n",
    "    ax.set_title(feat, fontsize=10)  # Title with a clean size\n",
    "\n",
    "# Remove empty subplots if there are more axes than features\n",
    "for ax in axes.flatten()[len(metric_features):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# Layout\n",
    "title = \"Numeric Variables' Histograms\"\n",
    "plt.suptitle(title)\n",
    "\n",
    "# Save the figure\n",
    "# plt.savefig(os.path.join('..', 'figures', 'eda', 'numeric_variables_histograms.png'), dpi=200)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sns.set() ## Reset to darkgrid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ALL Numeric Variables' Boxplots in one figure\n",
    "\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "# Calculate the number of rows and columns for a square matrix layout\n",
    "total_features = len(metric_features)\n",
    "sp_cols = math.ceil(math.sqrt(total_features))  # Columns based on the square root of total features\n",
    "sp_rows = math.ceil(total_features / sp_cols)   # Rows to accommodate all features\n",
    "\n",
    "# Prepare figure. Create individual axes where each boxplot will be placed\n",
    "fig, axes = plt.subplots(sp_rows, \n",
    "                         sp_cols, \n",
    "                         figsize=(20, 11),\n",
    "                         tight_layout=True\n",
    "                        )\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each boxplot. Hint: don't forget to attach the boxplot to the current ax\n",
    "for ax, feat in zip(axes.flatten(), metric_features): # Notice the zip() function and flatten() method\n",
    "    sns.boxplot(x=df_visualizations[feat], ax=ax)\n",
    "    ax.set_title(feat, fontsize=10)  # Set title\n",
    "    ax.xaxis.set_label_position('top')  # Move the label to the top\n",
    "    ax.set_xlabel('')  # Remove the default x-axis label at the bottom\n",
    "\n",
    "# Remove empty subplots if there are more axes than features\n",
    "for ax in axes.flatten()[len(metric_features):]:\n",
    "    fig.delaxes(ax)\n",
    "    \n",
    "# Layout\n",
    "title = \"Numeric Variables' Box Plots\"\n",
    "plt.suptitle(title)\n",
    "\n",
    "# Save the figure\n",
    "# plt.savefig(os.path.join('..', 'figures', 'eda', 'numeric_variables_boxplots.png'), dpi=200)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Reset theme to \"darkgrid\"\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Relationship of Numerical Variables excluding DOW and HR\n",
    "sns.set()\n",
    "\n",
    "# Setting pairplot\n",
    "sns.pairplot(df_visualizations[metric_features_excluding_dow_and_hr], diag_kind=\"hist\")\n",
    "\n",
    "# Layout\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.suptitle(\"Pairwise Relationship of Numerical Variables\", fontsize=20)\n",
    "    \n",
    "# plt.savefig(os.path.join('..', 'figures', 'eda', 'pairwise_numeric_scatterplots_excluding_dow_and_hr.png'), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of customer_age\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df_visualizations['customer_age'], bins=30, kde=True)\n",
    "plt.title('Distribution of Customer Age')\n",
    "plt.show()\n",
    "\n",
    "# Save the heatmap as an image in the figures directory\n",
    "# plt.savefig(os.path.join('..', 'figures', 'eda', 'correlation_matrix.png'), dpi=200)\n",
    "\n",
    "# average and median age\n",
    "print('The average age :', round(df_visualizations['customer_age'].mean(skipna=True)))\n",
    "print('The median age :', int(df_visualizations['customer_age'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features_excluding_dow_and_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the theme and style for the plot. 'ticks' removes gridlines but keeps ticks on axes\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "# Create a hexbin jointplot for 'customer_age' and 'CUI_Healthy'\n",
    "g = sns.jointplot(data=df_visualizations, \n",
    "                  x=\"customer_age\",   # X-axis: Customer Healthy\n",
    "                  y=\"CUI_Beverages\",  # Y-axis: CUI Beverages consumption\n",
    "                  kind=\"hex\",         # 'hex' creates hexagonal binning for density visualization\n",
    "                 )\n",
    "\n",
    "# Add a title to the entire figure, adjusting its vertical position with 'y'\n",
    "g.fig.suptitle(\"Hexagon Binning for Age and CUI Healthy\", y=1.03)\n",
    "\n",
    "# Set the axis labels\n",
    "g.set_axis_labels(\"Customer Age\", \"CUI Healthy\", fontsize=12)\n",
    "\n",
    "# Save the heatmap as an image in the figures directory\n",
    "# plt.savefig(os.path.join('..', 'figures', 'eda', 'correlation_matrix.png'), dpi=200)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair of Metric Variables Scatter Plot\n",
    "\n",
    "plt.scatter(df_visualizations[\"customer_age\"], df_visualizations[\"product_count\"], \n",
    "            edgecolors=\"black\", # Draw the edges of each point\n",
    "            alpha=.5, # change opacity level of each point\n",
    "            color='none' # remove the fill color of each point\n",
    "           )\n",
    "\n",
    "plt.xlabel(\"customer_age\")\n",
    "plt.ylabel(\"product_count\")\n",
    "\n",
    "# Save the heatmap as an image in the figures directory\n",
    "# plt.savefig(os.path.join('..', 'figures', 'eda', 'correlation_matrix.png'), dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a custom color using the cmap parameter: `cmap=\"\"`<br>\n",
    "**HINT** see default colormaps from matplotlib here: https://matplotlib.org/stable/users/explain/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "fig, ax = plt.subplots()\n",
    "cat1 = 'customer_age'\n",
    "cat2 = 'product_count'\n",
    "\n",
    "hb = ax.hexbin(df_visualizations[cat1], df_visualizations[cat2], \n",
    "               gridsize=20,\n",
    "               cmap='Blues'\n",
    "              )\n",
    "\n",
    "ax.set_title(\"{} and {},\\ncorr = {}\".format(cat1, cat2, \n",
    "                                            np.round(df_visualizations[['customer_age','CUI_Beverages']].corr().values[0,1],4)))\n",
    "ax.set_xlabel(cat1)\n",
    "ax.set_ylabel(cat2)\n",
    "\n",
    "cb = fig.colorbar(hb, ax=ax, label='counts')\n",
    "\n",
    "# Save the heatmap as an image in the figures directory\n",
    "# plt.savefig(os.path.join('..', 'figures', 'eda', 'correlation_matrix.png'), dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve the previous hexbin (original code already copied below)\n",
    "\n",
    "Specify a custom color using the cmap parameter: `cmap=\"\"`\n",
    "\n",
    "\n",
    "**HINT** see default colormaps from matplotlib here: https://matplotlib.org/stable/users/explain/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.7. Correlation Matrix <a class=\"anchor\" id=\"correlation_matrix\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare figure with a specified size for better clarity\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Compute the correlation matrix using the Pearson method for metric features (excluding DOW and HR columns)\n",
    "corr = df_visualizations[metric_features_excluding_dow_and_hr].corr(method=\"pearson\").round(2)\n",
    "\n",
    "# Create a mask to annotate only correlations that are very high or very low (above |0.5|)\n",
    "# Absolute value of the correlation matrix is checked to annotate values >= 0.5 or <= -0.5\n",
    "mask_annot = np.absolute(corr.values) >= 0.5\n",
    "\n",
    "# Annotate the heatmap: if the mask condition is True (|value| >= 0.5), display the correlation value; otherwise, leave it blank\n",
    "annot = np.where(mask_annot, corr.values, \"\")  # Blank cells for correlations not meeting the threshold\n",
    "\n",
    "# Plot heatmap of the correlation matrix\n",
    "sns.heatmap(data=corr, \n",
    "            annot=annot,         # Use custom annotations with the mask applied\n",
    "            fmt='s',             # Format annotations as strings to handle empty cells\n",
    "            vmin=-1, vmax=1,     # Set the color scale range for correlation (-1 to 1)\n",
    "            center=0,            # Center the colormap at 0 for balanced visual contrast\n",
    "            square=True,         # Make each cell a square shape\n",
    "            linewidths=0.5,      # Add thin lines between cells for better aesthetics\n",
    "            cmap='PiYG',         # Use a perceptually uniform diverging colormap (Purple to Yellow-Green)\n",
    "            cbar_kws={'shrink': .75} # Shrink the color bar to avoid overlapping with plot\n",
    "           )\n",
    "\n",
    "# Add a title to the heatmap\n",
    "plt.title(\"Correlation Matrix of Numeric Features (|corr| >= 0.5)\", fontsize=16)\n",
    "\n",
    "# Save the heatmap as an image in the figures directory\n",
    "# plt.savefig(os.path.join('..', 'figures', 'eda', 'correlation_matrix.png'), dpi=200)\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.8. Outliers <a class=\"anchor\" id=\"outliers\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_visualizations.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams[\"figure.figsize\"] = 25, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the histograms for each column\n",
    "axes = df[[\"CUI_American\", \"CUI_Asian\", \"CUI_Beverages\", \"CUI_Cafe\",\n",
    "                \"CUI_Chicken Dishes\", \"CUI_Chinese\", \"CUI_Desserts\", \"CUI_Healthy\",\n",
    "                \"CUI_Indian\", \"CUI_Italian\", \"CUI_Japanese\", \"CUI_Noodle Dishes\",\n",
    "                \"CUI_OTHER\", \"CUI_Street Food / Snacks\", \"CUI_Thai\",\"last_order\",\"product_count\"]].hist(color=\"cornflowerblue\", figsize=(15, 10))\n",
    "\n",
    "# Loop through each subplot\n",
    "for ax in axes.flatten():\n",
    "    for patch in ax.patches:\n",
    "        # Get the height of each bar (the count)\n",
    "        height = patch.get_height()\n",
    "        # Place text above each bar\n",
    "        ax.text(patch.get_x() + patch.get_width() / 2, height + 1,  # Offset text slightly above the bar\n",
    "                f\"{int(height)}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since almost every graph shows an outlier, let’s determine if this customer has high spending across all categories or if they differ significantly in specific Cui features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp= df[df[\"CUI_Asian\"]>600]\n",
    "df_temp[[\"CUI_American\", \"CUI_Asian\", \"CUI_Beverages\", \"CUI_Cafe\",\n",
    "                \"CUI_Chicken Dishes\", \"CUI_Chinese\", \"CUI_Desserts\", \"CUI_Healthy\",\n",
    "                \"CUI_Indian\", \"CUI_Italian\", \"CUI_Japanese\", \"CUI_Noodle Dishes\",\n",
    "                \"CUI_OTHER\", \"CUI_Street Food / Snacks\", \"CUI_Thai\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp= df[df[\"CUI_American\"]>200]\n",
    "df_temp[[\"CUI_American\", \"CUI_Asian\", \"CUI_Beverages\", \"CUI_Cafe\",\n",
    "                \"CUI_Chicken Dishes\", \"CUI_Chinese\", \"CUI_Desserts\", \"CUI_Healthy\",\n",
    "                \"CUI_Indian\", \"CUI_Italian\", \"CUI_Japanese\", \"CUI_Noodle Dishes\",\n",
    "                \"CUI_OTHER\", \"CUI_Street Food / Snacks\", \"CUI_Thai\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp= df[df[\"CUI_Chinese\"]>500]\n",
    "df_temp[[\"CUI_American\", \"CUI_Asian\", \"CUI_Beverages\", \"CUI_Cafe\",\n",
    "                \"CUI_Chicken Dishes\", \"CUI_Chinese\", \"CUI_Desserts\", \"CUI_Healthy\",\n",
    "                \"CUI_Indian\", \"CUI_Italian\", \"CUI_Japanese\", \"CUI_Noodle Dishes\",\n",
    "                \"CUI_OTHER\", \"CUI_Street Food / Snacks\", \"CUI_Thai\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see its different customers, so idk if we should remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = df[[\"customer_region\", \"customer_age\", \"vendor_count\", \"weekend_orders\", \"weekday_orders\", \"orders_dawn\",\"orders_morning\", \"orders_afternoon\", \"orders_evening\",\n",
    "       \"Healthiness_Index\",\"is_chain\"]].hist( color=\"cornflowerblue\")\n",
    "\n",
    "# Loop through each subplot\n",
    "for ax in axes.flatten():\n",
    "    for patch in ax.patches:\n",
    "        # Get the height of each bar (the count)\n",
    "        height = patch.get_height()\n",
    "        # Place text above each bar\n",
    "        ax.text(patch.get_x() + patch.get_width() / 2, height + 1,  # Offset text slightly above the bar\n",
    "                f\"{int(height)}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"orders_morning\"]>50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Function to create a bar plot with counts above each bar\n",
    "def plot_bar_with_counts(column, title):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    ax = sns.countplot(data=df, x=column, palette=\"Set3\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Count\")\n",
    "    \n",
    "    # Add count annotations above each bar\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{int(p.get_height())}\", \n",
    "                    (p.get_x() + p.get_width() / 2, p.get_height()), \n",
    "                    ha=\"center\", va=\"bottom\", fontsize=12, color=\"black\", xytext=(0, 5), \n",
    "                    textcoords=\"offset points\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot bar charts with counts\n",
    "plot_bar_with_counts(\"generation\", \"Distribution of Generation\")\n",
    "plot_bar_with_counts(\"last_promo\", \"Distribution of Last Promo\")\n",
    "plot_bar_with_counts(\"payment_method\", \"Distribution of Payment Method\")\n",
    "plot_bar_with_counts(\"used_last_promo\", \"Distribution of Used last Promo\")\n",
    "plot_bar_with_counts(\"customer_region\", \"Distribution of Customer Region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"feature_engineering\"></a>\n",
    "\n",
    "## 5. Feature Engineering <a class=\"anchor\" id=\"feature_engineering\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.1. Total Cuisine Spending <a class=\"anchor\" id=\"total_cuisine_spending\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>\n",
    "\n",
    "Estimate the customer lifetime value based on the total spending across all cuisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(like=\"CUI_\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns that start with \"CUI_\"\n",
    "cui_columns = df.filter(like=\"CUI_\").columns\n",
    "\n",
    "# Sum these columns row-wise and create a new column \"CUI_all\"\n",
    "df[\"total_cui_spending\"] = df[cui_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"total_cui_spending\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.2. Weekend and Weekday Orders <a class=\"anchor\" id=\"weekend_and_weekday_orders\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>\n",
    "\n",
    "Create a feature to indicate the number of orders placed during the weekend (Saturday and Sunday) versus weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for weekend orders (DOW_0 and DOW_6)\n",
    "df[\"weekend_orders\"] = df[\"DOW_0\"] + df[\"DOW_6\"]\n",
    "\n",
    "# Create a column for weekday orders (DOW_1 to DOW_5)\n",
    "df[\"weekday_orders\"] = df[[\"DOW_1\", \"DOW_2\", \"DOW_3\", \"DOW_4\", \"DOW_5\"]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"weekend_orders\", \"weekday_orders\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.3. Generation <a class=\"anchor\" id=\"generation\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>\n",
    "\n",
    "Create a categorical feature that groups customers by generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"customer_age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels for the generation categories\n",
    "bins = [14, 25, 41, 57, 76, 81]  # Bins must be set as one more than the number of categories\n",
    "labels = [\"Gen_Z\", \"Millennials\", \"Gen_X\", \"Baby_Boomers\", \"Silent_Generation\"]\n",
    "\n",
    "# Step 1: Create a new column for generation based on customer_age\n",
    "df[\"generation\"] = pd.cut(df[\"customer_age\"], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generation\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.4. Hourly Orders Aggregation by Time of Day <a class=\"anchor\" id=\"hourly_orders_aggregation\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>\n",
    "\n",
    "In this section, we analyze the distribution of customer orders across different time periods throughout the day. The hourly order data, ranging from midnight (HR_0) to 11 PM (HR_23), is aggregated into four distinct periods: Dawn (12 AM - 5 AM), Morning (6 AM - 11 AM), Afternoon (12 PM - 5 PM), and Evening (6 PM - 11 PM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hour ranges for each period\n",
    "dawn_hours = df.loc[:, \"HR_1\":\"HR_5\"].sum(axis=1)  # HR_1 to HR_5\n",
    "morning_hours = df.loc[:, \"HR_6\":\"HR_11\"].sum(axis=1)  # HR_6 to HR_11\n",
    "afternoon_hours = df.loc[:, \"HR_12\":\"HR_17\"].sum(axis=1)  # HR_12 to HR_17\n",
    "evening_hours = df.loc[:, \"HR_18\":\"HR_23\"].sum(axis=1)  # HR_18 to HR_23\n",
    "\n",
    "# Create new columns for each period\n",
    "df[\"orders_dawn\"] = dawn_hours\n",
    "df[\"orders_morning\"] = morning_hours\n",
    "df[\"orders_afternoon\"] = afternoon_hours\n",
    "df[\"orders_evening\"] = evening_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"orders_dawn\", \"orders_morning\", \"orders_afternoon\", \"orders_evening\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.5. Healthiness Index <a class=\"anchor\" id=\"healthiness_index\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"health_index\"] = df[\"CUI_Healthy\"] / df[\"total_cui_spending\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"health_index\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.6. Last Promotion Indicator <a class=\"anchor\" id=\"last_promotion_indicator\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>\n",
    "\n",
    "Create a binary feature indicating whether the customer used a promotion in their last order. This can be helpful for analysis on promotional effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['last_promo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['used_last_promo'] = (df['last_promo'] != 'NONE').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.7. Vendor Diversity <a class=\"anchor\" id=\"Vendor_Diversity\"></a>\n",
    "\n",
    "Measures the variety of vendors a customer orders from. Customers who order from many different vendors may have broader tastes or may be less loyal to any particular brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vendor_diversity\"] = df[\"vendor_count\"] / df[\"product_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vendor_diversity\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.8. Average Daily Orders <a class=\"anchor\" id=\"Average_Daily_Orders\"></a>\n",
    "\n",
    "How frequently does the customer place an order? This could help distinguish regular customers from occasional ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"total_orders\"] = df[[\"DOW_0\", \"DOW_1\", \"DOW_2\", \"DOW_3\", \"DOW_4\", \"DOW_5\", \"DOW_6\"]].sum(axis=1)\n",
    "\n",
    "df[\"avg_daily_orders\"] = (df[\"first_order\"] - df[\"last_order\"]) / df[\"total_orders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"avg_daily_orders\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.9. Order Recency <a class=\"anchor\" id=\"Order_Recency\"></a>\n",
    "\n",
    "How recent the customer’s last order is, normalized by the total duration of the dataset.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"order_recency\"] = df[\"last_order\"] / 90 # the dataset is about the last 3 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"order_recency\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ## 5.10. Average Order Value <a class=\"anchor\" id=\"Average_Order_Value\"></a>\n",
    "This feature could help in identifying high-value customers who spend more per order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"average_order_value\"] = df[\"total_cui_spending\"] / df[\"total_orders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"average_order_value\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['chain_perc'] = df['is_chain']/df['total_orders']\n",
    "\n",
    "# show histogram of chain_perc\n",
    "sns.histplot(df['chain_perc'], bins=10)\n",
    "plt.title('Distribution of chain_perc')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
