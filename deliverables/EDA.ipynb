{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<h3 align=\"center\">Data Mining 2024-25</h3>**\n",
    "## **<h3 align=\"center\">Customer Segmentation - ABCDEats Inc.</h3>**\n",
    "\n",
    "\n",
    "**Group 10 members:**<br>Alexandra Pinto - 20211599@novaims.unl.pt - 20211599<br>\n",
    "Marco Galão  - r20201545@novaims.unl.pt - r20201545<br>\n",
    "Sven Goerdes - 20240503@novaims.unl.pt - 20240503<br>\n",
    "Tim Straub  - 20240505@novaims.unl.pt - 20240505<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"toc\"></a>\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "* [1. Import the Libraries](#import_libraries)\n",
    "* [2. Import the Dataset](#import_dataset)\n",
    "* [3. Description of the Dataset’s Structure](#dataset_structure)\n",
    "* [4. Exploring the Dataset](#exploration)\n",
    "    * [4.1. Constant Features](#constant_features)\n",
    "    * [4.2. Duplicates](#duplicates)\n",
    "    * [4.3. Missing Values](#missing_values)\n",
    "    * [4.4. Data Types](#data_types)\n",
    "    * [4.5. Coherence Checking](#coherence_checking)\n",
    "        * [4.5.1. Minors Customers](#sub_section_4_5_1)\n",
    "        * [4.5.2. Is_chain variable](##sub_section_4_5_2)\n",
    "        * [4.5.3. CUI_Asian vs Japonese, Chinese](#sub_section_4_5_3)\n",
    "        * [4.5.4. Last_order and First_order consistency](#sub_section_4_5_4)\n",
    "        * [4.5.5. Sum DOWs vs sum HRs](##sub_section_4_5_5)\n",
    "        * [4.5.6. Total Orders and Vendor Count consistency ](#sub_section_4_5_6)\n",
    "        * [4.5.7. Total rows with inconsistencies](#sub_section_4_5_7)\n",
    "    * [4.6. Visualizations](#visualizations)\n",
    "    * [4.7. Correlation Matrix](#correlation_matrix)\n",
    "    * [4.8. Outliers](#outliers)\n",
    "* [5. Feature Engineering](#feature_engineering)\n",
    "    * [5.1. Generation](#generation)\n",
    "    * [5.2. Total Cuisine Spending](#total_cuisine_spending)\n",
    "    * [5.3. Healthiness Index](#healthiness_index)\n",
    "    * [5.4. Weekend and Weekday Orders](#weekend_and_weekday_orders)\n",
    "    * [5.5. Hourly Orders Aggregation](#hourly_orders_aggregation)\n",
    "    * [5.6. Order Recency](#Order_Recency)\n",
    "    * [5.7. Average Daily Orders](#Average_Daily_Orders)\n",
    "    * [5.8. Average Order Value](#Average_Order_Value)\n",
    "    * [5.9. Last Promotion Indicator](#last_promotion_indicator)\n",
    "    * [5.10. Vendor Diversity](#Vendor_Diversity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1. Import the Libraries <a class=\"anchor\" id=\"import_libraries\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "For tasks involving data manipulation, numerical calculations, visualization, and machine learning, imports of libraries like the mentioned in the below cell are crucial. These libraries offer the required functions and tools for preprocessing data, pattern analysis, consumer segmentation, and development of focused marketing campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2. Import the Dataset <a class=\"anchor\" id=\"import_dataset\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "In this section, we import the datasets and set the customer_id as the index column in DM2425_ABCDEats_DATASET. Also, check the first and last 5 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"../Data/DM2425_ABCDEats_DATASET.csv\", index_col=\"customer_id\")\n",
    "df_original.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.tail().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Description of the Dataset’s Structure<a class=\"anchor\" id=\"dataset_structure\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "Here, let's try to identify missing values, check if data types are appropriate and performs summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The <span style=\"color:Blue\"> info()  </span> method prints information about the DataFrame. Using this method we can evaluate the following:\n",
    "\n",
    "Missing values:\n",
    "- customer_age\n",
    "- first_order\n",
    "- HR_0\n",
    "\n",
    "Data types:\n",
    "- customer_age should be a integer\n",
    "- first_order is number of days so should be a integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "The <span style=\"color:Blue\"> describe()  </span> method  is used for calculating some statistical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.describe().round(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the HR_0 is always equal to zero, so we can delete this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the unique values that exist in categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_original[\"customer_region\"].value_counts(normalize=True) * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_original[\"last_promo\"].value_counts(normalize=True) * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets replace the - for None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_original[\"payment_method\"].value_counts(normalize=True) * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are hidden missing values in \"customer_region\" (\"-\") and we should evaluate what could mean \"-\" in \"last_promo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"exploration\"></a>\n",
    "\n",
    "# 4. Exploring the Dataset <a class=\"anchor\" id=\"a\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "In this section we perform several checks and treatments to ensure data quality. <br>\n",
    "This entails locating and managing duplicates to prevent redundant data, removing constant characteristics that don't offer insightful information, and dealing with missing values using methods like imputation or exclusion. These procedures are essential for preserving data integrity and enhancing the accuracy of later analysis and modeling efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.1. Constant features <a class=\"anchor\" id=\"constant_features\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>\n",
    "\n",
    "The <span style='color:Blue'> var() </span> method allows us to check if there are any of the numerical variables are univariate (variance is equal to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = df_original.var(numeric_only=True)\n",
    "variances[variances == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the univariate feature \"HR_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_hr0 = df_original.drop(\"HR_0\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.2. Duplicates <a class=\"anchor\" id=\"duplicates\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>\n",
    "\n",
    "With the <span style=\"color:Blue\"> drop_duplicates() </span> method the duplicates rows will be dropped. We can conclude that that this dataset had 60 duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicated indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_hr0.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates = df_no_hr0[~df_no_hr0.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicated rows (without considering indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows removed\n",
    "rows_removed = len(df_no_hr0) - len(df_no_duplicates)\n",
    "\n",
    "# Calculate the percentage of rows removed\n",
    "percent_removed = (rows_removed / len(df_no_hr0)) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"Rows removed due to duplicates: {rows_removed} ({round(percent_removed, 2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.3. Missing values <a class=\"anchor\" id=\"missing_values\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we concluded there are some features that have some missing values. In this subsection we will check for each feature and try to understand what would be the best approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing = df_no_duplicates.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of disguised missing values \"-\" in each column\n",
    "perc_dash_values = (df_treat_missing.apply(lambda x: (x == '-').sum()) / len(df_treat_missing) * 100).round(2)\n",
    "perc_dash_values[perc_dash_values > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values in each column\n",
    "perc_missing_values = (df_treat_missing.isna().sum() / len(df_treat_missing) * 100).round(2)\n",
    "perc_missing_values[perc_missing_values > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**last_promo**\n",
    "\n",
    "Evaluate \"-\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_treat_missing[\"last_promo\"].value_counts(normalize=True, ascending=False) * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new columns (and auxiliary variables with column names) to help creating visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New columns\n",
    "\n",
    "# Select all columns that start with \"CUI_\"\n",
    "cui_columns = df_treat_missing.filter(like=\"CUI_\").columns.tolist()\n",
    "dow_columns = df_treat_missing.filter(like=\"DOW_\").columns.tolist()\n",
    "hr_columns = df_treat_missing.filter(like=\"HR_\").columns.tolist()\n",
    "\n",
    "# Fill NaN values with 0 before summing\n",
    "df_treat_missing[\"total_cui_spending\"] = df_treat_missing[cui_columns].sum(axis=1)\n",
    "df_treat_missing[\"total_orders\"] = df_treat_missing[dow_columns].sum(axis=1)\n",
    "\n",
    "# Auxiliary variables\n",
    "\n",
    "# Define metric and non-metric features\n",
    "metric_features = df_treat_missing.select_dtypes(include=['number']).columns.tolist()\n",
    "non_metric_features = df_treat_missing.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "# Exclude DOW and HR (and CUI) columns from metric features\n",
    "metric_features_excluding_dow_and_hr = [feat for feat in metric_features if feat not in dow_columns + hr_columns]\n",
    "metric_features_excluding_cui_dow_and_hr = [feat for feat in metric_features if feat not in cui_columns + dow_columns + hr_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "To improve the visualizations and maintain coherence across all of them, let's define a consistent color palette that will be applied throughout.\n",
    "\n",
    "https://seaborn.pydata.org/tutorial/color_palettes.html#sequential-color-brewer-palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set color palette\n",
    "color_palette = \"Set2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style and color palette\n",
    "sns.set()\n",
    "sns.set_palette(color_palette)  # Consistent color palette across all plots\n",
    "\n",
    "# Define order for last_promo categories based on frequency\n",
    "last_promo_order = df_treat_missing['last_promo'].value_counts().index.tolist()\n",
    "\n",
    "# Separate numeric and categorical variables\n",
    "numeric_vars = df_treat_missing.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_vars = df_treat_missing.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "# Select variables to plot, excluding 'last_promo', CUI, DOW, and HR variables\n",
    "variables_to_plot = [col for col in df_treat_missing.columns if col != 'last_promo' and not (col.startswith(\"CUI\") or col.startswith(\"DOW\") or col.startswith(\"HR\"))]\n",
    "\n",
    "# Calculate grid layout for subplots\n",
    "total_features = len(variables_to_plot) + 2  # +2 for 'last_promo' and 'customer_region'\n",
    "sp_rows = math.ceil(math.sqrt(total_features))\n",
    "sp_cols = math.ceil(total_features / sp_rows)\n",
    "\n",
    "# Prepare figure with specified grid layout\n",
    "fig, axes = plt.subplots(sp_rows, sp_cols, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 'last_promo' countplot with specified order\n",
    "sns.countplot(data=df_treat_missing, x='last_promo', ax=axes[0], order=last_promo_order, palette=color_palette)\n",
    "axes[0].set(xlabel=\"last_promo\", ylabel=\"Count\")\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0, ha='center')  # Horizontal labels\n",
    "\n",
    "# Loop through variables to plot each one\n",
    "for ax, feat in zip(axes[1:], variables_to_plot):\n",
    "    if feat == \"customer_region\":\n",
    "        sns.countplot(data=df_treat_missing, x=feat, hue='last_promo', ax=ax, hue_order=last_promo_order)\n",
    "        ax.set(xlabel=\"customer_region\", ylabel=\"Count\")\n",
    "        ax.legend(title=\"last_promo\", loc='upper right')  # Move legend inside the plot\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha='center')  # Vertical labels\n",
    "    elif feat in numeric_vars:\n",
    "        sns.histplot(df_treat_missing, x=feat, ax=ax, bins=10, hue='last_promo', kde=True, hue_order=last_promo_order)\n",
    "        ax.set(ylabel=\"Density\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha='center')  # Horizontal labels\n",
    "    else:\n",
    "        # For other categorical variables (\"payment_method\"), plot stacked bar chart\n",
    "        stacked_data = df_treat_missing.groupby(['last_promo', feat]).size().unstack(fill_value=0)\n",
    "        stacked_data = stacked_data.loc[last_promo_order]  # Reorder rows to match last_promo order\n",
    "        stacked_data.div(stacked_data.sum(axis=1), axis=0).plot(kind='bar', stacked=True, ax=ax)\n",
    "        ax.set(xlabel=\"last_promo\", ylabel=\"Proportion\")\n",
    "        ax.legend(title=feat, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha='center')  # Horizontal labels\n",
    "\n",
    "# Plot cuisine variables separately if they exist\n",
    "cuisine_columns = [col for col in df_treat_missing.columns if col.startswith('CUI_')]\n",
    "if cuisine_columns:\n",
    "    cuisine_data = df_treat_missing.groupby('last_promo')[cuisine_columns].sum()\n",
    "    cuisine_data = cuisine_data.loc[last_promo_order]  # Ensure the last_promo order is maintained\n",
    "    cuisine_proportion = cuisine_data.div(cuisine_data.sum(axis=1), axis=0)\n",
    "    \n",
    "    # Select top 5 cuisines, others combined as 'Not Top 5'\n",
    "    top_cuisines = cuisine_proportion.sum().nlargest(5).index\n",
    "    combined_cuisines = cuisine_proportion[top_cuisines].copy()\n",
    "    combined_cuisines['Not Top 5'] = cuisine_proportion.drop(top_cuisines, axis=1).sum(axis=1)\n",
    "    \n",
    "    # Plot combined cuisine data as stacked bar chart\n",
    "    ax = axes[len(variables_to_plot) + 1]\n",
    "    combined_cuisines.plot(kind='bar', stacked=True, ax=ax)\n",
    "    ax.set(xlabel=\"last_promo\", ylabel=\"Proportion\")\n",
    "    ax.legend(title='CUI_', bbox_to_anchor=(1, 1), loc='upper left')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha='center')  # Horizontal labels\n",
    "\n",
    "# Remove empty subplots\n",
    "for ax in axes[len(variables_to_plot) + 2:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# Layout and display\n",
    "plt.suptitle(\"Distribution of Variables by Last Promotion Category\", y=1.01, fontsize=16, weight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** All \"last_promo\" categories, including those labeled with “-”, display similar distribution shapes. This consistency across distributions suggests that the records marked with “-” are not random and likely represent a specific group of customers rather than missing data. Therefore, we can infer that they indicate customers who did not engage with any promotions. By doing so, we allow for more effective segmentation and insights into customer behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing[\"last_promo\"].replace(\"-\", 'NOPROMO', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**customer_region**\n",
    "\n",
    "Notes:\n",
    "- Values could represent postal codes\n",
    "- Check if customer region values with similar prefix indicate geographical proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_treat_missing[\"customer_region\"].value_counts(normalize=True, ascending=False) * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style and color palette\n",
    "sns.set()\n",
    "sns.set_palette(color_palette)  # Consistent color palette across all plots\n",
    "\n",
    "# Define order for customer_region categories based on frequency\n",
    "customer_region_order = df_treat_missing['customer_region'].value_counts().index.tolist()\n",
    "\n",
    "# Separate \"-\" from the rest and sort the others numerically in descending order\n",
    "if \"-\" in customer_region_order:\n",
    "    customer_region_order = [\"-\"] + sorted([region for region in customer_region_order if region != \"-\"], key=lambda x: int(x), reverse=True)\n",
    "\n",
    "# Separate numeric and categorical variables\n",
    "numeric_vars = df_treat_missing.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_vars = df_treat_missing.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "# Select variables to plot, excluding 'customer_region', CUI, DOW, and HR variables\n",
    "variables_to_plot = [col for col in df_treat_missing.columns if col != 'customer_region' and not (col.startswith(\"CUI\") or col.startswith(\"DOW\") or col.startswith(\"HR\"))]\n",
    "\n",
    "# Calculate grid layout for subplots\n",
    "total_features = len(variables_to_plot) + 2  # +2 for 'customer_region' and 'last_promo'\n",
    "sp_rows = math.ceil(math.sqrt(total_features))\n",
    "sp_cols = math.ceil(total_features / sp_rows)\n",
    "\n",
    "# Prepare figure with specified grid layout\n",
    "fig, axes = plt.subplots(sp_rows, sp_cols, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 'customer_region' countplot with specified order\n",
    "sns.countplot(data=df_treat_missing, x='customer_region', ax=axes[0], order=df_treat_missing['customer_region'].value_counts().index.tolist(), palette=color_palette)\n",
    "axes[0].set(xlabel=\"customer_region\", ylabel=\"Count\")\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0, ha='center')  # Horizontal labels\n",
    "\n",
    "# Loop through variables to plot each one\n",
    "for ax, feat in zip(axes[1:], variables_to_plot):\n",
    "    if feat in numeric_vars:\n",
    "        sns.histplot(df_treat_missing, x=feat, ax=ax, bins=10, hue='customer_region', kde=True, hue_order=customer_region_order)\n",
    "        ax.set(ylabel=\"Density\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha='center')  # Horizontal labels\n",
    "    else:\n",
    "        # For other categorical variables, plot stacked bar chart\n",
    "        stacked_data = df_treat_missing.groupby(['customer_region', feat]).size().unstack(fill_value=0)\n",
    "        stacked_data = stacked_data.loc[customer_region_order]  # Reorder rows to match customer_region order\n",
    "        stacked_data.div(stacked_data.sum(axis=1), axis=0).plot(kind='bar', stacked=True, ax=ax)\n",
    "        ax.set(xlabel=\"customer_region\", ylabel=\"Proportion\")\n",
    "        ax.legend(title=feat, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha='center')  # Horizontal labels\n",
    "\n",
    "# Plot cuisine variables separately\n",
    "cuisine_columns = [col for col in df_treat_missing.columns if col.startswith('CUI_')]\n",
    "if cuisine_columns:\n",
    "    cuisine_data = df_treat_missing.groupby('customer_region')[cuisine_columns].sum()\n",
    "    cuisine_data = cuisine_data.loc[customer_region_order]  # Ensure the customer_region order is maintained\n",
    "    cuisine_proportion = cuisine_data.div(cuisine_data.sum(axis=1), axis=0)\n",
    "    \n",
    "    # Select top 5 cuisines, others combined as 'Not Top 5'\n",
    "    top_cuisines = cuisine_proportion.sum().nlargest(5).index\n",
    "    combined_cuisines = cuisine_proportion[top_cuisines].copy()\n",
    "    combined_cuisines['Not Top 5'] = cuisine_proportion.drop(top_cuisines, axis=1).sum(axis=1)\n",
    "    \n",
    "    # Plot combined cuisine data as stacked bar chart\n",
    "    ax = axes[len(variables_to_plot) + 1]\n",
    "    combined_cuisines.plot(kind='bar', stacked=True, ax=ax)\n",
    "    ax.set(xlabel=\"customer_region\", ylabel=\"Proportion\")\n",
    "    ax.legend(title='CUI_', bbox_to_anchor=(1, 1), loc='upper left')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha='center')  # Horizontal labels\n",
    "\n",
    "# Remove empty subplots\n",
    "for ax in axes[len(variables_to_plot) + 2:]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "# Layout and display\n",
    "plt.suptitle(\"Distribution of Variables by Customer Region\", y=1.01, fontsize=16, weight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** After analyzing the plots, it appears that customer region values with similar prefixes indicate geographical proximity, as they exhibit similar behavior and statistics. This observation aligns with the project description, which notes that the data was collected from three cities. \n",
    "\n",
    "1. **Group 1:**\n",
    "   - 8670\n",
    "   - 8550\n",
    "   - 8370\n",
    "\n",
    "2. **Group 2:**\n",
    "   - 4660\n",
    "   - 4140\n",
    "\n",
    "3. **Group 3:**\n",
    "   - 2490\n",
    "   - 2440\n",
    "   - 2360\n",
    "\n",
    "Additionally, the customer region represented by `\"-\"` shares more similarities with the regions `8670` and `8370`. \n",
    "\n",
    "**Next steps:** The next step is to identify the customer region that is most similar to `\"-\"` (`8670` or `8370`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style and color palette\n",
    "sns.set()\n",
    "sns.set_palette(color_palette)  # Consistent color palette across all plots\n",
    "\n",
    "# Define relevant regions\n",
    "regions = ['-', '8670', '8370']\n",
    "\n",
    "# Filter DataFrame for the selected regions\n",
    "df_selected = df_treat_missing[df_treat_missing['customer_region'].isin(regions)]\n",
    "\n",
    "# Sum spending for each cuisine type per region\n",
    "region_spending = df_selected.groupby('customer_region')[cui_columns].sum()\n",
    "\n",
    "# Calculate total spending percentages for each cuisine type by region\n",
    "region_percentages = region_spending.div(region_spending.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Create a comparison DataFrame for the top 5 cuisines per region\n",
    "top_cuisine_percentages = {}\n",
    "\n",
    "for region in regions:\n",
    "    top_cuisines = region_spending.loc[region].nlargest(5).index\n",
    "    top_percentages = region_percentages.loc[region, top_cuisines]\n",
    "    top_cuisine_percentages[region] = top_percentages\n",
    "\n",
    "# Convert to DataFrame\n",
    "comparison_df = pd.DataFrame(top_cuisine_percentages).reset_index().melt(id_vars='index', var_name='customer_region', value_name='percentage')\n",
    "\n",
    "# Plot using Seaborn's countplot to visualize the top cuisines by region\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=comparison_df, x='index', y='percentage', hue='customer_region', palette=color_palette, alpha=1.0)\n",
    "plt.title('Comparison of Spending Percentages for Top 5 Cuisines Across Selected Regions', fontsize=16, weight='bold')\n",
    "plt.ylabel('Percentage of Total Spending')\n",
    "plt.xlabel('')\n",
    "plt.xticks(rotation=0, ha='center')\n",
    "plt.legend(title='customer_region', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Layout and display\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization suggests that customers from the empty region `-` are geographically close to those in region `8670`. Therefore, we will reassign the region for all customers classified under `-` to `8670`.\n",
    "\n",
    "**Note:** In the future, we will use KNN to replace the values represented by \"-\" with the mode of the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing['customer_region'].replace('-', '8670', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**customer_age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values in \"customer_age\"\n",
    "perc_missing_customer_age = (df_treat_missing[\"customer_age\"].isna().sum() / len(df_treat_missing) * 100).round(2)\n",
    "\n",
    "# Print percentage of missing values\n",
    "print(f\"{perc_missing_customer_age}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_age = df_treat_missing[df_treat_missing['customer_age'].isna()]\n",
    "nan_age[metric_features_excluding_cui_dow_and_hr].describe().round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_age.describe(include='object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style\n",
    "sns.set()\n",
    "\n",
    "# Assuming 'df_treat_missing' contains the relevant data and 'customer_age' is one of the columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the histogram\n",
    "sns.histplot(df_treat_missing['customer_age'], bins=10, kde=True, alpha=0.6)\n",
    "\n",
    "# Calculate the median age and add a vertical line\n",
    "median_age = df_treat_missing['customer_age'].median()\n",
    "plt.axvline(median_age, color='red', linestyle='--', linewidth=2, label=f'Median Age: {round(median_age)}')\n",
    "\n",
    "# Set titles and labels\n",
    "plt.title('Distribution of Customer Age', fontsize=16, weight='bold')\n",
    "plt.xlabel('Customer Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The analysis of descriptive statistics for the relevant variables revealed no significant standout features. With only 2.28% of data missing, we will impute the missing customer ages using the median (the median is robust to outliers and provides a more accurate reflection of a typical value, especially in skewed distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing['customer_age'].fillna(median_age, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future:** Try with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# # Make a copy of the dataframe\n",
    "# df_kNN_impute = df_treat_missing.copy()\n",
    "\n",
    "# # Store the original index\n",
    "# original_index = df_kNN_impute.index\n",
    "\n",
    "# # Get categorical columns\n",
    "# categorical_columns = df_kNN_impute.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# # Dictionary to store label encoders\n",
    "# label_encoders = {}\n",
    "\n",
    "# # Encode categorical variables\n",
    "# for col in categorical_columns:\n",
    "#     # Convert categorical column to string type first\n",
    "#     df_kNN_impute[col] = df_kNN_impute[col].astype(str)\n",
    "    \n",
    "#     # Replace 'nan' strings with 'MISSING'\n",
    "#     df_kNN_impute[col] = df_kNN_impute[col].replace('nan', 'MISSING')\n",
    "    \n",
    "#     le = LabelEncoder()\n",
    "#     df_kNN_impute[col] = le.fit_transform(df_kNN_impute[col])\n",
    "#     label_encoders[col] = le\n",
    "\n",
    "# # Scale numerical columns\n",
    "# numerical_columns = df_kNN_impute.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# scaler = StandardScaler()\n",
    "# df_kNN_impute[numerical_columns] = scaler.fit_transform(df_kNN_impute[numerical_columns])\n",
    "\n",
    "# # Initialize and fit KNN imputer\n",
    "# imputer = KNNImputer(n_neighbors=5)\n",
    "# imputed_array = imputer.fit_transform(df_kNN_impute)\n",
    "\n",
    "# # Convert back to dataframe with original index\n",
    "# df_imputed = pd.DataFrame(imputed_array, columns=df_kNN_impute.columns, index=original_index)\n",
    "\n",
    "# # Inverse transform numerical columns\n",
    "# df_imputed[numerical_columns] = scaler.inverse_transform(df_imputed[numerical_columns])\n",
    "\n",
    "# # Inverse transform categorical columns\n",
    "# for col in categorical_columns:\n",
    "#     df_imputed[col] = label_encoders[col].inverse_transform(df_imputed[col].astype(int))\n",
    "#     # If the original column was categorical, convert back to category type\n",
    "#     if df_treat_missing[col].dtype.name == 'category':\n",
    "#         df_imputed[col] = pd.Categorical(df_imputed[col], categories=df_treat_missing[col].cat.categories)\n",
    "\n",
    "# # Now let's compare the values before and after imputation\n",
    "# print(\"Original missing values:\", df_treat_missing['customer_age'].isna().sum())\n",
    "# print(\"Remaining missing values:\", df_imputed['customer_age'].isna().sum())\n",
    "\n",
    "# # Get the original indices where customer_age was NaN\n",
    "# nan_indices = df_treat_missing[df_treat_missing['customer_age'].isna()].index\n",
    "\n",
    "# # Create a comparison DataFrame\n",
    "# comparison_df = pd.DataFrame({\n",
    "#     'Original': df_treat_missing['customer_age'][nan_indices],\n",
    "#     'Imputed': df_imputed['customer_age'][nan_indices]\n",
    "# })\n",
    "\n",
    "# # Round imputed values to 2 decimal places for better readability\n",
    "# comparison_df['Imputed'] = comparison_df['Imputed'].round(2)\n",
    "\n",
    "# # Show the comparison\n",
    "# print(\"\\nComparison of NaN values before and after imputation:\")\n",
    "# print(comparison_df)\n",
    "\n",
    "# df_treat_missing['customer_age'] = df_imputed['customer_age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**first_order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values in \"first_order\"\n",
    "perc_missing_first_order = (df_treat_missing[\"first_order\"].isna().sum() / len(df_treat_missing) * 100).round(2)\n",
    "\n",
    "# Print percentage of missing values\n",
    "print(f\"{perc_missing_first_order}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_first_order = df_treat_missing[df_treat_missing['first_order'].isna()]\n",
    "nan_first_order[metric_features_excluding_cui_dow_and_hr].describe().round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_first_order.describe(include='object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame based on 'vendor_count' condition\n",
    "filtered_nan_first_order = nan_first_order[nan_first_order['total_orders'] != 1]\n",
    "\n",
    "# Select the relevant features the DataFrame\n",
    "filtered_nan_first_order[metric_features_excluding_cui_dow_and_hr + dow_columns].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of those customers in the DataFrame\n",
    "percentage_filtered = len(filtered_nan_first_order) / len(df_treat_missing) * 100\n",
    "\n",
    "# Print percentage\n",
    "print(f\"Filtered customers make up {round(percentage_filtered, 2)}% of the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Since most customers with missing first_order data have last_order = 0 and total_orders = 1, we know they placed only one order on the first day of the dataset. Therefore, we can confidently impute first_order as 0 for these cases.\n",
    "\n",
    "For the two customers with total_orders = 2, while they placed their orders on DOW_6 with two vendors, it’s possible that these orders occurred across different Saturdays. Given this minor ambiguity affects only 0.01% of the data, we can remove these cases without significantly impacting the dataset’s integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update `first_order` with `last_order` for all customers with missing `first_order`\n",
    "df_treat_missing.loc[nan_first_order.index, 'first_order'] = df_treat_missing.loc[nan_first_order.index, 'last_order']\n",
    "\n",
    "# Identify customers, with missing `first_order`, where `total_orders` is different than 1\n",
    "customers_to_remove = nan_first_order[nan_first_order['total_orders'] != 1].index\n",
    "\n",
    "# Drop the two customers with missing `first_order` and `total_orders` different than 1\n",
    "df_treat_missing.drop(index=customers_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treat_missing.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows removed due to missing values\n",
    "rows_removed_due_to_missing = len(df_no_duplicates) - len(df_treat_missing)\n",
    "\n",
    "# Calculate the percentage of rows removed\n",
    "percent_rows_removed_due_to_missing = rows_removed_due_to_missing / len(df_no_duplicates) * 100\n",
    "\n",
    "# Calculate the number of values imputed\n",
    "values_imputed = df_no_duplicates.isna().sum().sum() - df_treat_missing.isna().sum().sum()\n",
    "percent_values_imputed = (values_imputed / df_treat_missing.size) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"Rows removed due to missing values: {rows_removed_due_to_missing} ({round(percent_rows_removed_due_to_missing, 2)}%)\")\n",
    "print(f\"Values imputed due to missing data: {values_imputed} ({round(percent_values_imputed, 2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.4. Data Types <a class=\"anchor\" id=\"data_types\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_dtypes = df_treat_missing.copy()\n",
    "df_adjust_dtypes.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- age: should be integer\n",
    "- first order: is number of days so should also be integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using \"Int64\" allows for the representation of NaN values alongside integers\n",
    "df_adjust_dtypes[\"customer_age\"] = df_adjust_dtypes[\"customer_age\"].astype(\"Int64\")\n",
    "df_adjust_dtypes[\"first_order\"] = df_adjust_dtypes[\"first_order\"].astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.5. Coherence Checking <a class=\"anchor\" id=\"coherence_checking\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>\n",
    "\n",
    "> ### 4.5.1. Minors Customers <a class=\"anchor\" id=\"sub_section_4_5_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inconsistencies = df_adjust_dtypes.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if there are minor customers (should not exist, because they are not allowed to placed orders online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the age distribution\n",
    "df_inconsistencies[\"customer_age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count customers under 18\n",
    "minor_customers = df_inconsistencies[df_inconsistencies[\"customer_age\"] < 18]\n",
    "perc_minor_customers = len(minor_customers) / len(df_inconsistencies) * 100\n",
    "\n",
    "print(f\"Number of customers under 18 years old: {len(minor_customers)} ({round(perc_minor_customers, 2)}%)\")\n",
    "\n",
    "# Show descriptive statistics\n",
    "minor_customers[metric_features_excluding_cui_dow_and_hr].describe().round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_customers.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Since only 1.15% of customers in the dataset are minors, who are not legally permitted to place online orders, we can remove these entries. Excluding minors from the dataset will improve the accuracy and relevance of our analysis for the target demographic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove customers under 18 from the dataset\n",
    "df_inconsistencies = df_inconsistencies[df_inconsistencies[\"customer_age\"] >= 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.4. Last_order and First_order consistency <a class=\"anchor\" id=\"sub_section_4_5_4\"></a>\n",
    "\n",
    "Verify that last_order is lower than or equal to first_order for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in \"first_order\" (\"last_order\" has no missing values)\n",
    "data_cleaned = df_inconsistencies.dropna(subset=\"first_order\")\n",
    "\n",
    "# Check when \"last_order\" is greater than \"first_order\"\n",
    "is_last_order_lower = data_cleaned[\"last_order\"] < data_cleaned[\"first_order\"]\n",
    "\n",
    "# Calculate the percentage of inconsistent rows based on the original dataframe\n",
    "perc_inconsistencies = is_last_order_lower.sum() / len(df_inconsistencies) * 100\n",
    "\n",
    "# Output the percentage of rows where \"last_order\" is greater than \"first_order\"\n",
    "print(f\"Inconsistent rows ('last_order' < 'first_order'): {perc_inconsistencies.round(2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.3. CUI_Asian vs Chinese, Indian, Japanese, etc <a class=\"anchor\" id=\"sub_section_4_5_3\"></a>\n",
    "\n",
    "The classification of restaurants into broad and specific cuisine categories ('CUI_Asian' alongside 'CUI_Asian', 'CUI_Chinese', 'CUI_Indian', 'CUI_Japanese', 'CUI_Noodle Dishes', 'CUI_Thai') could lead to inconsistencies and overlaps in data. Some restaurants may be categorized under both a broad category ('CUI_Asian') and a specific one (e.g., 'CUI_Chinese'), while others might only fall under the broader classification. This double classification could result in redundancy, making it challenging to analyze customer preferences and spending habits accurately.\n",
    "\n",
    "To explore this issue, we can analyze the following:\n",
    "\n",
    "- Correlation Analysis: How strongly correlated are the specific categories with the broader CUI_Asian category?\n",
    "- Frequency of Overlap: How often do entries fall under both CUI_Asian and specific categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cui_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cuisine categories to analyze\n",
    "asian_cui = ['CUI_Asian', 'CUI_Chinese', 'CUI_Indian', 'CUI_Japanese', 'CUI_Noodle Dishes', 'CUI_Thai']\n",
    "\n",
    "# Create a binary DataFrame indicating whether there is spending on each cuisine\n",
    "cuisine_presence = df_inconsistencies[asian_cui].gt(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seaborn theme and color palette for consistency\n",
    "sns.set()\n",
    "\n",
    "# Correlation Analysis between CUI_Asian and specific categories\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = df_inconsistencies[asian_cui].corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            fmt=\".2f\", \n",
    "            linewidths=0.5,  # Add lines between cells for clarity\n",
    "            linecolor='black',  # Color of the lines\n",
    "            cbar_kws={\"shrink\": .8})  # Adjust colorbar size\n",
    "\n",
    "# Set labels and title\n",
    "plt.title('Correlation Matrix: CUI_Asian and Specific Cuisines', fontsize=16, weight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x labels for better readability\n",
    "plt.yticks(rotation=0, fontsize=12)  # Keep y labels horizontal\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of entries with spending on each specific cuisine and CUI_Asian\n",
    "overlap_counts = cuisine_presence[cuisine_presence['CUI_Asian'] == 1].sum()\n",
    "\n",
    "# Create a comparison DataFrame with overlap counts and total non-zero counts for each specific cuisine\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Overlap_Count': overlap_counts[1:],  # Get overlap counts for specific cuisines\n",
    "    'Non_Zero_Count': (cuisine_presence[asian_cui[1:]].sum()).values  # Sum for non-zero counts\n",
    "})\n",
    "\n",
    "# Calculate total spending for each cuisine\n",
    "total_spending = df_inconsistencies[asian_cui].sum()\n",
    "\n",
    "# Add total spending to the comparison DataFrame\n",
    "comparison_df['Total_Spending'] = total_spending[1:]  # Total spending for specific cuisines\n",
    "\n",
    "# Calculate the percentage spent on each specific cuisine relative to CUI_Asian spending in overlapping cases\n",
    "cui_asian_spending = total_spending['CUI_Asian']\n",
    "comparison_df['Overlap_Spending_Percentage'] = (comparison_df['Total_Spending'] / cui_asian_spending) * 100\n",
    "\n",
    "# Calculate the proportion of overlap compared to total non-zero counts\n",
    "comparison_df['Overlap_Count_Percentage'] = (comparison_df['Overlap_Count'] / comparison_df['Non_Zero_Count']) * 100\n",
    "\n",
    "# Replace any potential division by zero with NaN for clarity\n",
    "comparison_df['Overlap_Spending_Percentage'].replace([float('inf'), -float('inf')], pd.NA, inplace=True)\n",
    "comparison_df['Overlap_Count_Percentage'].replace([float('inf'), -float('inf')], pd.NA, inplace=True)\n",
    "\n",
    "# Sorting the DataFrame by Overlap_Count_Percentage in descending order\n",
    "comparison_df.sort_values(by='Overlap_Count_Percentage', ascending=False, inplace=True)\n",
    "\n",
    "# Display the improved DataFrame\n",
    "comparison_df[['Overlap_Count_Percentage', 'Overlap_Spending_Percentage']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific Asian cuisines\n",
    "specific_cuisines = asian_cui[1:]\n",
    "\n",
    "# Create an auxiliary DataFrame from the original\n",
    "auxiliary_df = df_inconsistencies.copy()\n",
    "\n",
    "# Calculate total spending for specific cuisines in the auxiliary DataFrame\n",
    "auxiliary_df['Total_Specific_Cuisines'] = auxiliary_df[specific_cuisines].sum(axis=1)\n",
    "\n",
    "# Check if the totals match (CUI_Asian from original DataFrame)\n",
    "auxiliary_df['CUI_Match'] = auxiliary_df['CUI_Asian'] == auxiliary_df['Total_Specific_Cuisines']\n",
    "\n",
    "# Display rows where the sums do not match\n",
    "mismatched_entries = auxiliary_df[~auxiliary_df['CUI_Match']]\n",
    "\n",
    "# Calculate the percentage of mismatched entries\n",
    "mismatched_percentage = len(mismatched_entries) / len(auxiliary_df) * 100\n",
    "\n",
    "# Output the results including percentage\n",
    "print(f\"Found {len(mismatched_entries)} ({mismatched_percentage:.2f}%) mismatched entries.\")\n",
    "\n",
    "mismatched_entries[['CUI_Asian', 'Total_Specific_Cuisines']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The analysis revealed 20,792 mismatched entries, representing approximately 66.09% of the dataset. This substantial discrepancy indicates inconsistencies in the categorization of restaurants. Specifically, some restaurants are classified solely under the broad category of CUI_Asian, while others are tagged under both CUI_Asian and one or more specific cuisine categories. Therefore, mismatched entries don't necessarily indicate an error but rather a complex categorization approach that requires careful interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.5. Sum DOWs vs sum HRs <a class=\"anchor\" id=\"sub_section_4_5_5\"></a>\n",
    "\n",
    "Check that the sum of orders by day (DOW_0 to DOW_6) matches the total number of orders from hourly data (HR_1 to HR_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of DOW columns\n",
    "dow_sum = df_inconsistencies[dow_columns].sum(axis=1)\n",
    "\n",
    "# Sum of HR columns\n",
    "hr_sum = df_inconsistencies[hr_columns].sum(axis=1)\n",
    "\n",
    "# Calculate the absolute difference between the sums of DOW and HR columns\n",
    "sums_diff = (dow_sum - hr_sum).abs()\n",
    "\n",
    "# Calculate the percentage of inconsistent rows (for printing purposes)\n",
    "perc_rows_inconsistencies = (sums_diff > 0).mean() * 100\n",
    "\n",
    "# Calculate the percentage of rows for each unique difference value\n",
    "diff_counts = sums_diff.value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "# Calculate the cumulative percentage\n",
    "cumulative_percentage = diff_counts.cumsum()\n",
    "\n",
    "# Create a DataFrame to display the percentage of rows with each deviation and cumulative percentages\n",
    "results_df = pd.DataFrame({\n",
    "    'Rows (%)': diff_counts.round(2),\n",
    "    'Cumulative Rows (%)': cumulative_percentage.round(2)\n",
    "})\n",
    "\n",
    "# Set the index to the deviation (difference between DOW and HR sums)\n",
    "results_df.index.name = 'Deviation (dow_sum - hr_sum)'\n",
    "\n",
    "# Print the results with descriptions\n",
    "print(f\"Percentage of inconsistent rows: {perc_rows_inconsistencies.round(2)}%\")\n",
    "\n",
    "print(\"\\nPercentage of differences between DOW and HR sums:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style and color palette to match previous visualizations\n",
    "sns.set()\n",
    "\n",
    "# Data for Deviation and Cumulative Rows Percentage\n",
    "deviations = results_df.index  # Deviation (dow_sum - hr_sum)\n",
    "cumulative_rows = results_df['Cumulative Rows (%)']  # Cumulative Rows (%)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=deviations, y=cumulative_rows, marker='o', color='b')\n",
    "\n",
    "# Titles and labels\n",
    "plt.title(\"Cumulative Percentage of Rows by Deviation Between dow_sum and hr_sum\", fontsize=16, weight='bold')\n",
    "plt.xlabel(\"Deviation (dow_sum - hr_sum)\", fontsize=12)\n",
    "plt.ylabel(\"Cumulative Rows (%)\", fontsize=12)\n",
    "\n",
    "# Improve x-axis ticks to show all deviation values\n",
    "plt.xticks(ticks=deviations)\n",
    "\n",
    "# Add value labels on top of each point for clarity\n",
    "for x, y in zip(deviations, cumulative_rows):\n",
    "    plt.text(x, y + 0.08, f\"{y:.2f}\", ha='center', fontsize=9)\n",
    "\n",
    "# Grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Layout adjustments\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed approach to handle these inconsistencies:\n",
    "\n",
    "Small Inconsistencies (1-2 Orders):\n",
    "- Carefully investigate patterns or potential causes, as these inconsistencies make up the majority of the discrepancies\n",
    "- If no clear pattern is found, we will consider creating an aggregate feature (e.g., total_orders) that captures the sum of orders from either DOW or HR columns, to ensure consistency\n",
    "\n",
    "Large Inconsistencies (3+ Orders):\n",
    "- Investigate these cases in more detail to identify any potential issues\n",
    "- If no clear pattern is found, we will remove these rows to maintain data integrity without heavily impacting the analysis, as they represent a small portion of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyze the small inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows where the sums are inconsistent\n",
    "df_small_inconsistencies = df_inconsistencies[(sums_diff == 1) | (sums_diff == 2)]\n",
    "\n",
    "# Get summary statistics of the inconsistent rows\n",
    "df_small_inconsistencies.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics of the small inconsistent rows (for categorical variables)\n",
    "df_small_inconsistencies.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small_inconsistencies.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Small Inconsistencies (1-2 Orders): No clear pattern was found, so we will consider creating an aggregate feature (e.g., total_orders) that captures the sum of orders from either DOW or HR columns, to ensure consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyze the large inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows where the sums are inconsistent\n",
    "df_large_inconsistencies = df_inconsistencies[sums_diff > 2]\n",
    "\n",
    "# Get summary statistics of the inconsistent rows\n",
    "df_large_inconsistencies.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics of the small inconsistent rows (for categorical variables)\n",
    "df_large_inconsistencies.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_inconsistencies.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Large Inconsistencies (3+ Orders): No clear pattern was found, so let's remove these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ows with inconsistencies greater than 2\n",
    "df_inconsistencies = df_inconsistencies[~(sums_diff > 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.2. Is_chain variable <a class=\"anchor\" id=\"sub_section_4_5_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inconsistencies[\"is_chain\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* is_chain is described, in the metadata, as a boolean variable that indicates whether the customer’s order was from a chain restaurant.\n",
    "     * This can't be true as one row describes one customer \n",
    "     * --> Hypothesis: Tracks the number of orders a customer placed from a chain restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style and color palette\n",
    "sns.set()\n",
    "\n",
    "# create a new variable/series that tracks the the gap between total orders and is_chain\n",
    "gap = df_inconsistencies['total_orders'] - df_inconsistencies['is_chain']\n",
    "\n",
    "# Define bin edges and labels to cover a wider range of gaps more intuitively\n",
    "bin_edges = [-float('inf'), -1, 0, 1, 2, 3, 5, 10, 20, 50, float('inf')]\n",
    "bin_labels = [\"-1\", \"0\", \"1\", \"2\", \"3\", \"4-5\", \"6-10\", \"11-20\", \"21-50\", \">50\"]\n",
    "\n",
    "# Bin the data using pd.cut with the new edges and labels\n",
    "gap_binned = pd.cut(gap, bins=bin_edges, labels=bin_labels)\n",
    "\n",
    "# Create a DataFrame with the binned values and count occurrences\n",
    "gap_val_binned = pd.DataFrame(gap_binned.value_counts().sort_index(), columns=['count'])\n",
    "gap_val_binned.index.name = 'gap'\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=gap_val_binned.index, y=gap_val_binned['count'])\n",
    "\n",
    "# Add value labels on top of each bar with a larger font size and a bit more space above each bar\n",
    "for idx, value in enumerate(gap_val_binned['count']):\n",
    "    plt.text(x=idx, y=value + 2, s=f\"{value}\", ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Gap between total_orders and is_chain', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Frequency Distribution of Gaps between total_orders and is_chain (Binned)', \n",
    "          fontsize=16, weight='bold')\n",
    "\n",
    "# Layout and display\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Based on our previous analysis, the variable \"is_chain\" appears to represent the number of orders a customer has placed from chain restaurants, considering the observed range and distribution of the gaps. To enhance our understanding of customer behavior, we will create a **new feature called chain_orders_prop**, which will capture the frequency of a customer’s orders from chain restaurants as a proportion of their total orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column 'is_chain' to 'chain_orders'\n",
    "df_inconsistencies.rename(columns={'is_chain': 'chain_orders'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine auxiliary variables, because \"is_chain\" was renamed to \"chain_orders\"\n",
    "\n",
    "# Define metric and non-metric features\n",
    "metric_features = df_inconsistencies.select_dtypes(include=['number']).columns.tolist()\n",
    "non_metric_features = df_inconsistencies.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "# Exclude DOW and HR (and CUI) columns from metric features\n",
    "metric_features_excluding_dow_and_hr = [feat for feat in metric_features if feat not in dow_columns + hr_columns]\n",
    "metric_features_excluding_cui_dow_and_hr = [feat for feat in metric_features if feat not in cui_columns + dow_columns + hr_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze cases where \"chain_orders\" is greater than \"total_orders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows where chain_orders is greater than total_orders\n",
    "inconsistencies = df_inconsistencies[df_inconsistencies['chain_orders'] > df_inconsistencies['total_orders']]\n",
    "\n",
    "# Calculate the percentage of inconsistent rows\n",
    "percentage_inconsistencies = len(inconsistencies) / len(df_inconsistencies) * 100\n",
    "\n",
    "# Display the percentage of inconsistent rows\n",
    "print(f\"Rows where 'chain_orders' exceeds 'total_orders': {len(inconsistencies)} ({percentage_inconsistencies:.2f}%)\")\n",
    "\n",
    "# Display the inconsistent rows summary\n",
    "inconsistencies[metric_features_excluding_cui_dow_and_hr].describe().round(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** We found 74 entries (0.24%) where chain_orders exceed total_orders. These records are customers who have not placed any orders in the last three months. Therefore, we can set their chain_orders to match total_orders, which is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle these observations, by setting \"chain_orders\" to \"total_orders\"\n",
    "df_inconsistencies.loc[df_inconsistencies['chain_orders'] > df_inconsistencies['total_orders'], 'chain_orders'] = df_inconsistencies['total_orders']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.6. Total Orders and Vendor Count consistency <a class=\"anchor\" id=\"sub_section_4_5_6\"></a>\n",
    "\n",
    "Total Orders Consistency: The total number of orders placed (the sum of the DOW/HR columns) should not exceed the total number of products ordered (product_count).<br>\n",
    "\n",
    "Vendor Count Consistency: The vendor_count should not exceed the number of unique products ordered, since each product should come from a vendor. This ensures that the recorded count of unique vendors matches the reality of product purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total orders based on DOW columns\n",
    "total_orders = df_inconsistencies[dow_columns].sum(axis=1)\n",
    "\n",
    "# Check if total orders are less than product count\n",
    "inconsistent_orders = total_orders > df_inconsistencies['product_count']\n",
    "\n",
    "# Check if vendor count exceeds product count\n",
    "inconsistent_vendors = df_inconsistencies['vendor_count'] > df_inconsistencies['product_count']\n",
    "\n",
    "# Combine both conditions to find all inconsistencies\n",
    "inconsistencies_mask = inconsistent_orders | inconsistent_vendors\n",
    "\n",
    "# Count the number of inconsistent rows\n",
    "num_inconsistent_rows = inconsistencies_mask.sum()\n",
    "\n",
    "# Calculate the percentage of inconsistent rows\n",
    "perc_inconsistent_rows = num_inconsistent_rows / len(df_inconsistencies) * 100\n",
    "\n",
    "# Display results\n",
    "print(f\"Number of inconsistent rows: {num_inconsistent_rows} ({perc_inconsistent_rows.round(2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the rows with inconsistencies in orders and vendors are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the indices of both inconsistency sets are the same\n",
    "inconsistent_rows_same = inconsistent_orders.index.equals(inconsistent_vendors.index)\n",
    "\n",
    "# Step 2: Output the result\n",
    "if inconsistent_rows_same:\n",
    "    print(\"The rows with inconsistencies in orders and vendors are the same.\")\n",
    "else:\n",
    "    print(\"There are differences in the rows with inconsistencies between orders and vendors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inconsistencies[inconsistencies_mask].head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with inconsistencies in total orders or vendor count\n",
    "df_inconsistencies = df_inconsistencies[~inconsistencies_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.5.7. Total rows with inconsistencies <a class=\"anchor\" id=\"sub_section_4_5_7\"></a>\n",
    "Final Step: Calculate the Percentage of Rows Removed Due to Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows removed\n",
    "rows_removed_inconsistencies = len(df_adjust_dtypes) - len(df_inconsistencies)\n",
    "rows_removed_missing_and_inconsistencies = len(df_no_duplicates) - len(df_inconsistencies)\n",
    "\n",
    "# Calculate the percentage of rows removed\n",
    "percentage_removed_inconsistencies = rows_removed_inconsistencies / len(df_adjust_dtypes) * 100\n",
    "percentage_removed_missing_and_inconsistencies = rows_removed_missing_and_inconsistencies / len(df_no_duplicates) * 100\n",
    "\n",
    "# Output the result\n",
    "print(f\"Rows removed due to inconsistencies: {rows_removed_inconsistencies} ({round(percentage_removed_inconsistencies, 2)}%)\")\n",
    "print(f\"Rows removed due to missing values and inconsistencies: {rows_removed_missing_and_inconsistencies} ({round(percentage_removed_missing_and_inconsistencies, 2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** We removed 454 rows (1.43%) due to inconsistencies (and an additional 2 rows due to missing values). This slight reduction in data maintains the overall integrity of the dataset, enhancing its reliability for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.6. Visualizations <a class=\"anchor\" id=\"visualizations\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default colormaps from matplotlib: https://matplotlib.org/stable/users/explain/colors/colormaps.html\n",
    "\n",
    "Matplotlib tutorials: https://matplotlib.org/stable/tutorials/index.html\n",
    "\n",
    "Matplotlib gallery: https://matplotlib.org/stable/gallery/index.html\n",
    "\n",
    "Seaborn tutorials: https://seaborn.pydata.org/tutorial.html\n",
    "\n",
    "Seaborn gallery: https://seaborn.pydata.org/examples/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualizations = df_inconsistencies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualizations.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.7. Correlation Matrix <a class=\"anchor\" id=\"correlation_matrix\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seaborn theme\n",
    "sns.set()\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = df_visualizations[metric_features_excluding_cui_dow_and_hr].corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask, \n",
    "            annot=True, \n",
    "            cmap='PiYG', \n",
    "            fmt=\".2f\", \n",
    "            linewidths=0.5,  # Add lines between cells for clarity\n",
    "            linecolor='black',  # Color of the lines\n",
    "            cbar_kws={\"shrink\": .8})  # Adjust colorbar size\n",
    "\n",
    "# Set labels and title\n",
    "plt.title('Correlation Matrix', fontsize=16, weight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x labels for better readability\n",
    "plt.yticks(rotation=0, fontsize=12)  # Keep y labels horizontal\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 4.8. Outliers <a class=\"anchor\" id=\"outliers\"></a>\n",
    "[Back to 4. Exploring the Dataset](#exploration)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = df_visualizations.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams[\"figure.figsize\"] = 25, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the histograms for each column\n",
    "axes = df_outliers[[\"CUI_American\", \"CUI_Asian\", \"CUI_Beverages\", \"CUI_Cafe\",\n",
    "                \"CUI_Chicken Dishes\", \"CUI_Chinese\", \"CUI_Desserts\", \"CUI_Healthy\",\n",
    "                \"CUI_Indian\", \"CUI_Italian\", \"CUI_Japanese\", \"CUI_Noodle Dishes\",\n",
    "                \"CUI_OTHER\", \"CUI_Street Food / Snacks\", \"CUI_Thai\",\"last_order\",\"product_count\"]].hist(color=\"cornflowerblue\", figsize=(15, 10))\n",
    "\n",
    "# Loop through each subplot\n",
    "for ax in axes.flatten():\n",
    "    for patch in ax.patches:\n",
    "        # Get the height of each bar (the count)\n",
    "        height = patch.get_height()\n",
    "        # Place text above each bar\n",
    "        ax.text(patch.get_x() + patch.get_width() / 2, height + 1,  # Offset text slightly above the bar\n",
    "                f\"{int(height)}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since almost every graph shows an outlier, let’s determine if this customer has high spending across all categories or if they differ significantly in specific Cui features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp= df_outliers[df_outliers[\"CUI_Asian\"]>600]\n",
    "df_temp[[\"CUI_American\", \"CUI_Asian\", \"CUI_Beverages\", \"CUI_Cafe\",\n",
    "                \"CUI_Chicken Dishes\", \"CUI_Chinese\", \"CUI_Desserts\", \"CUI_Healthy\",\n",
    "                \"CUI_Indian\", \"CUI_Italian\", \"CUI_Japanese\", \"CUI_Noodle Dishes\",\n",
    "                \"CUI_OTHER\", \"CUI_Street Food / Snacks\", \"CUI_Thai\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp= df_outliers[df_outliers[\"CUI_American\"]>200]\n",
    "df_temp[[\"CUI_American\", \"CUI_Asian\", \"CUI_Beverages\", \"CUI_Cafe\",\n",
    "                \"CUI_Chicken Dishes\", \"CUI_Chinese\", \"CUI_Desserts\", \"CUI_Healthy\",\n",
    "                \"CUI_Indian\", \"CUI_Italian\", \"CUI_Japanese\", \"CUI_Noodle Dishes\",\n",
    "                \"CUI_OTHER\", \"CUI_Street Food / Snacks\", \"CUI_Thai\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp= df_outliers[df_outliers[\"CUI_Chinese\"]>500]\n",
    "df_temp[[\"CUI_American\", \"CUI_Asian\", \"CUI_Beverages\", \"CUI_Cafe\",\n",
    "                \"CUI_Chicken Dishes\", \"CUI_Chinese\", \"CUI_Desserts\", \"CUI_Healthy\",\n",
    "                \"CUI_Indian\", \"CUI_Italian\", \"CUI_Japanese\", \"CUI_Noodle Dishes\",\n",
    "                \"CUI_OTHER\", \"CUI_Street Food / Snacks\", \"CUI_Thai\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see its different customers, so idk if we should remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axes = df_outliers[[\"customer_region\", \"customer_age\", \"vendor_count\", \"weekend_orders\", \"weekday_orders\", \"orders_dawn\",\"orders_morning\", \"orders_afternoon\", \"orders_evening\",\n",
    "#        \"Healthiness_Index\",\"is_chain\"]].hist( color=\"cornflowerblue\")\n",
    "\n",
    "# # Loop through each subplot\n",
    "# for ax in axes.flatten():\n",
    "#     for patch in ax.patches:\n",
    "#         # Get the height of each bar (the count)\n",
    "#         height = patch.get_height()\n",
    "#         # Place text above each bar\n",
    "#         ax.text(patch.get_x() + patch.get_width() / 2, height + 1,  # Offset text slightly above the bar\n",
    "#                 f\"{int(height)}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_outliers[df_outliers[\"orders_morning\"]>50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_outliers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# # Function to create a bar plot with counts above each bar\n",
    "# def plot_bar_with_counts(column, title):\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     ax = sns.countplot(data=df_outliers, x=column, palette=\"Set3\")\n",
    "#     plt.title(title)\n",
    "#     plt.ylabel(\"Count\")\n",
    "    \n",
    "#     # Add count annotations above each bar\n",
    "#     for p in ax.patches:\n",
    "#         ax.annotate(f\"{int(p.get_height())}\", \n",
    "#                     (p.get_x() + p.get_width() / 2, p.get_height()), \n",
    "#                     ha=\"center\", va=\"bottom\", fontsize=12, color=\"black\", xytext=(0, 5), \n",
    "#                     textcoords=\"offset points\")\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Plot bar charts with counts\n",
    "# plot_bar_with_counts(\"generation\", \"Distribution of Generation\")\n",
    "# plot_bar_with_counts(\"last_promo\", \"Distribution of Last Promo\")\n",
    "# plot_bar_with_counts(\"payment_method\", \"Distribution of Payment Method\")\n",
    "# plot_bar_with_counts(\"used_last_promo\", \"Distribution of Used last Promo\")\n",
    "# plot_bar_with_counts(\"customer_region\", \"Distribution of Customer Region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"feature_engineering\"></a>\n",
    "\n",
    "## 5. Feature Engineering <a class=\"anchor\" id=\"feature_engineering\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features = df_outliers.copy()\n",
    "df_new_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.1. Generation <a class=\"anchor\" id=\"generation\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>\n",
    "\n",
    "Create a categorical feature that groups customers by generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels for the generation categories\n",
    "bins = [18, 25, 41, 57, 76, 81]  # Bins must be set as one more than the number of categories\n",
    "labels = [\"Gen_Z\", \"Millennials\", \"Gen_X\", \"Baby_Boomers\", \"Silent_Generation\"]\n",
    "\n",
    "# Step 1: Create a new column for generation based on customer_age\n",
    "df_new_features[\"generation\"] = pd.cut(df_new_features[\"customer_age\"], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df_new_features['generation'].value_counts(normalize=True) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.2. Total Cuisine Spending <a class=\"anchor\" id=\"total_cuisine_spending\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>\n",
    "\n",
    "Estimate the customer lifetime value based on the total spending across all cuisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum these columns row-wise and create a new column \"CUI_all\"\n",
    "df_new_features[\"total_cui_spending\"] = df_new_features[cui_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features[\"total_cui_spending\"].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.3. Healthiness Index <a class=\"anchor\" id=\"healthiness_index\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features[\"health_index\"] = df_new_features[\"CUI_Healthy\"] / df_new_features[\"total_cui_spending\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features[\"health_index\"].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.4. Weekend and Weekday Orders <a class=\"anchor\" id=\"weekend_and_weekday_orders\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>\n",
    "\n",
    "Create a feature to indicate the number of orders placed during the weekend (Saturday and Sunday) versus weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for weekend orders (DOW_0 and DOW_6)\n",
    "df_new_features[\"weekend_orders\"] = df_new_features[\"DOW_0\"] + df_new_features[\"DOW_6\"]\n",
    "\n",
    "# Create a column for weekday orders (DOW_1 to DOW_5)\n",
    "df_new_features[\"weekday_orders\"] = df_new_features.loc[:, \"DOW_1\":\"DOW_5\"].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features[[\"weekend_orders\", \"weekday_orders\"]].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.5. Hourly Orders Aggregation by Time of Day <a class=\"anchor\" id=\"hourly_orders_aggregation\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>\n",
    "\n",
    "In this section, we analyze the distribution of customer orders across different time periods throughout the day. The hourly order data, ranging from midnight (HR_0) to 11 PM (HR_23), is aggregated into four distinct periods: Dawn (12 AM - 5 AM), Morning (6 AM - 11 AM), Afternoon (12 PM - 5 PM), and Evening (6 PM - 11 PM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hour ranges for each period\n",
    "dawn_hours = df_new_features.loc[:, \"HR_1\":\"HR_5\"].sum(axis=1)  # HR_1 to HR_5\n",
    "morning_hours = df_new_features.loc[:, \"HR_6\":\"HR_11\"].sum(axis=1)  # HR_6 to HR_11\n",
    "afternoon_hours = df_new_features.loc[:, \"HR_12\":\"HR_17\"].sum(axis=1)  # HR_12 to HR_17\n",
    "evening_hours = df_new_features.loc[:, \"HR_18\":\"HR_23\"].sum(axis=1)  # HR_18 to HR_23\n",
    "\n",
    "# Create new columns for each period\n",
    "df_new_features[\"orders_dawn\"] = dawn_hours\n",
    "df_new_features[\"orders_morning\"] = morning_hours\n",
    "df_new_features[\"orders_afternoon\"] = afternoon_hours\n",
    "df_new_features[\"orders_evening\"] = evening_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features.loc[:, \"orders_dawn\":\"orders_evening\"].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.6. Order Recency <a class=\"anchor\" id=\"Order_Recency\"></a>\n",
    "\n",
    "Recency of the customer’s last order, normalized by the full time span covered in the dataset.\n",
    "\n",
    "While last_order gives direct information, order_recency will simplify interpreting recency at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the recency of the last order: 1 being the most recent and 0 being the least recent\n",
    "df_new_features[\"order_recency\"] = df_new_features[\"last_order\"] / df_new_features['last_order'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features[\"order_recency\"].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.7. Average Daily Orders <a class=\"anchor\" id=\"Average_Daily_Orders\"></a>\n",
    "\n",
    "How frequently does the customer place an order? This could help distinguish regular customers from occasional ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average daily orders based on the total dataset period (from first day to most recent order in the dataset)\n",
    "df_new_features[\"avg_daily_orders\"] = df_new_features[\"total_orders\"] / df_new_features['last_order'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features[\"avg_daily_orders\"].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8. Average Order Value <a class=\"anchor\" id=\"Average_Order_Value\"></a>\n",
    "This feature could help in identifying high-value customers who spend more per order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features[\"avg_order_value\"] = df_new_features[\"total_cui_spending\"] / df_new_features[\"total_orders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features[\"avg_order_value\"].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 5.9. Promotion Used Indicator <a class=\"anchor\" id=\"last_promotion_indicator\"></a>\n",
    "[Back to 5. Feature Engineering](#feature_engineering)<br>\n",
    "\n",
    "Create a binary feature indicating whether the customer used a promotion. This can be helpful for analysis on promotional effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df_new_features['last_promo'].value_counts(normalize=True) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicates if a promotion was used based on the last_promo value\n",
    "df_new_features['promo_used'] = (df_new_features['last_promo'] != 'NOPROMO').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df_new_features['promo_used'].value_counts(normalize=True) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ## 5.10 Chain Orders Proportion <a class=\"anchor\" id=\"Chain_Percentage_Chain_Customer\"></a>\n",
    "\n",
    "Tracks how much percentage of orders are Chain Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of orders that are from chain vendors (handling potential division by zero)\n",
    "df_new_features['chain_orders_prop'] = df_new_features['chain_orders'] / df_new_features['total_orders'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_features[\"chain_orders_prop\"].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"feature_engineering\"></a>\n",
    "\n",
    "## 6. Visualizations for new Features <a class=\"anchor\" id=\"feature_engineering\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the starting column 'total_cui_spending'\n",
    "start_index = df_new_features.columns.get_loc('total_cui_spending')\n",
    "\n",
    "# Get column names from 'total_cui_spending' to the end\n",
    "new_features = df_new_features.columns[start_index:]\n",
    "\n",
    "# Display the selected column names\n",
    "new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total_cui_spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set consistent style and color\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Histogram for 'Total_cui_spending'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_new_features['total_cui_spending'], kde=True, color=\"skyblue\")\n",
    "plt.xlim(left = 0, right= 400)\n",
    "plt.title(\"Distribution of Total CUI Spending\")\n",
    "plt.xlabel(\"Total CUI Spending\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# # Scatter plot for 'Total_cui_spending' vs. 'customer_age' (as an example numerical comparison)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.scatterplot(x='customer_age', y='Total_cui_spending', data=df_new_features, color=\"skyblue\")\n",
    "# plt.title(\"Total CUI Spending vs Customer Age\")\n",
    "# plt.xlabel(\"Customer Age\")\n",
    "# plt.ylabel(\"Total CUI Spending\")\n",
    "# plt.show()\n",
    "\n",
    "# Horizontal box plot for 'Total_cui_spending'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=df_new_features['total_cui_spending'], color=\"skyblue\")\n",
    "plt.title(\"Box Plot of Total CUI Spending\")\n",
    "plt.xlabel(\"Total CUI Spending\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weekend and Weekday_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set consistent style and color\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Count plot for 'weekend_orders'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='weekend_orders', data=df_new_features, color=\"skyblue\")\n",
    "plt.title(\"Count of Weekend Orders\")\n",
    "plt.xlabel(\"Number of Weekend Orders\")\n",
    "plt.xlim(left = 0, right= 10)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Count plot for 'weekday_orders'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='weekday_orders', data=df_new_features, color=\"skyblue\")\n",
    "plt.title(\"Count of Weekday Orders\")\n",
    "plt.xlabel(\"Number of Weekday Orders\")\n",
    "plt.xlim(left = 0, right= 25)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot for 'generation' with total counts displayed\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x='generation', data=df_new_features, color=\"skyblue\", order=labels)\n",
    "plt.title(\"Count of Generations\")\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Add total counts above each bar\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='edge', padding=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hourly Orders Aggregation by Time of Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram and Box Plot for 'orders_dawn'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_new_features['orders_dawn'], kde=True, color=\"skyblue\")\n",
    "plt.title(\"Distribution of Dawn Orders\")\n",
    "plt.xlabel(\"Number of Dawn Orders\")\n",
    "plt.xlim(left = 0, right= 10)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=df_new_features['orders_dawn'], color=\"skyblue\")\n",
    "plt.title(\"Box Plot of Dawn Orders\")\n",
    "plt.xlabel(\"Number of Dawn Orders\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram and Box Plot for 'orders_morning'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_new_features['orders_morning'], kde=True, color=\"skyblue\")\n",
    "plt.title(\"Distribution of Morning Orders\")\n",
    "plt.xlabel(\"Number of Morning Orders\")\n",
    "plt.xlim(left = 0, right= 10)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=df_new_features['orders_morning'], color=\"skyblue\")\n",
    "plt.title(\"Box Plot of Morning Orders\")\n",
    "plt.xlabel(\"Number of Morning Orders\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram and Box Plot for 'orders_afternoon'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_new_features['orders_afternoon'], kde=True, color=\"skyblue\")\n",
    "plt.title(\"Distribution of Afternoon Orders\")\n",
    "plt.xlabel(\"Number of Afternoon Orders\")\n",
    "plt.xlim(left = 0, right= 10)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=df_new_features['orders_afternoon'], color=\"skyblue\")\n",
    "plt.title(\"Box Plot of Afternoon Orders\")\n",
    "plt.xlabel(\"Number of Afternoon Orders\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram and Box Plot for 'orders_evening'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_new_features['orders_evening'], kde=True, color=\"skyblue\")\n",
    "plt.title(\"Distribution of Evening Orders\")\n",
    "plt.xlabel(\"Number of Evening Orders\")\n",
    "plt.xlim(left = 0, right= 10)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=df_new_features['orders_evening'], color=\"skyblue\")\n",
    "plt.title(\"Box Plot of Evening Orders\")\n",
    "plt.xlabel(\"Number of Evening Orders\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter Plots Comparing Different Order Times\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='orders_dawn', y='orders_morning', data=df_new_features, color=\"skyblue\")\n",
    "plt.title(\"Orders Morning vs Orders Dawn\")\n",
    "plt.xlabel(\"Dawn Orders\")\n",
    "plt.xlim(left = 0, right= 10)\n",
    "plt.ylabel(\"Morning Orders\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='orders_dawn', y='orders_afternoon', data=df_new_features, color=\"skyblue\")\n",
    "plt.title(\"Orders Afternoon vs Orders Dawn\")\n",
    "plt.xlabel(\"Dawn Orders\")\n",
    "plt.ylabel(\"Afternoon Orders\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='orders_dawn', y='orders_evening', data=df_new_features, color=\"skyblue\")\n",
    "plt.title(\"Orders Evening vs Orders Dawn\")\n",
    "plt.xlabel(\"Dawn Orders\")\n",
    "plt.ylabel(\"Evening Orders\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='orders_morning', y='orders_afternoon', data=df_new_features, color=\"skyblue\")\n",
    "plt.title(\"Orders Afternoon vs Orders Morning\")\n",
    "plt.xlabel(\"Morning Orders\")\n",
    "plt.ylabel(\"Afternoon Orders\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='orders_morning', y='orders_evening', data=df_new_features, color=\"skyblue\")\n",
    "plt.title(\"Orders Evening vs Orders Morning\")\n",
    "plt.xlabel(\"Morning Orders\")\n",
    "plt.ylabel(\"Evening Orders\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='orders_afternoon', y='orders_evening', data=df_new_features, color=\"skyblue\")\n",
    "plt.title(\"Orders Evening vs Orders Afternoon\")\n",
    "plt.xlabel(\"Afternoon Orders\")\n",
    "plt.ylabel(\"Evening Orders\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Healthiness_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for 'health_index'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df_new_features['health_index'], kde=True, color=\"skyblue\")\n",
    "plt.title(\"Distribution of Health Index\")\n",
    "plt.xlabel(\"Health Index\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for 'health_index' vs 'total_cui_spending'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='total_cui_spending', y='health_index', data=df_new_features, color=\"skyblue\")\n",
    "plt.title(\"Health Index vs Total CUI Spending\")\n",
    "plt.xlabel(\"Total CUI Spending\")\n",
    "plt.ylabel(\"Health Index\")\n",
    "plt.show()\n",
    "\n",
    "# Box plot for 'health_index'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=df_new_features['health_index'], color=\"skyblue\")\n",
    "plt.title(\"Box Plot of Health Index\")\n",
    "plt.xlabel(\"Health Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promo_used_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart for 'promo_used' with total counts displayed inside the bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x='promo_used', data=df_new_features, color=\"skyblue\")\n",
    "plt.title(\"Promotion Usage Indicator\")\n",
    "plt.xlabel(\"Promotion Used (1 = Yes, 0 = No)\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Add total count labels inside the bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add here (for all variables):\n",
    "- histograms (numeric variables)\n",
    "- countplots (categorical variables)\n",
    "- scatter plots (numeric variables)\n",
    "- boxplots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
